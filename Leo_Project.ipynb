{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/earendil94/SMLExam/blob/master/Leo_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLD803bQHlrZ",
        "colab_type": "text"
      },
      "source": [
        "**STATISTICAL MACHINE LEARNING**\n",
        "\n",
        "ARRIGHI Leonardo, BRAND Francesco, DORIGO Claudia\n",
        "\n",
        "\n",
        "Dataset folder is saved in \"/content/drive/My Drive/SML/SML_Project\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADn7VmfFHe_a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "574243d7-2eae-4552-d7a0-d69567c8bb74"
      },
      "source": [
        "# link colab and drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "# then follow passages"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqQgtn1AuSAi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "92995064-1eb5-4d2e-dddf-cddba3c4952b"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from IPython import display\n",
        "\n",
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "torch.manual_seed(160898)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Device: {}'.format(device))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eycNavNlqLPg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b4ddae88-03ef-445e-908d-8a08f3c06e65"
      },
      "source": [
        "csv = '/content/drive/My Drive/SML/SML_Project/Chunk1/results.csv'\n",
        "imgs = '/content/drive/My\\ Drive/SML/SML_Project/Chunk1/img1_200/'\n",
        "\n",
        "!ls {imgs} | wc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    200     200    2382\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmvRFfu92TsE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "outputId": "b410259e-1356-49a7-8102-b1217c44fc29"
      },
      "source": [
        "# Load csv\n",
        "df = pd.read_csv(csv, delimiter='|') \n",
        "print(df.shape)\n",
        "print(df.columns[2], df.columns[2] == ' comment') \n",
        "df[' comment'].values[0]\n",
        "df.head(6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(158915, 3)\n",
            " comment True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>comment_number</th>\n",
              "      <th>comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000092795.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>Two young guys with shaggy hair look at their...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000092795.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>Two young , White males are outside near many...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000092795.jpg</td>\n",
              "      <td>2</td>\n",
              "      <td>Two men in green shirts are standing in a yard .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000092795.jpg</td>\n",
              "      <td>3</td>\n",
              "      <td>A man in a blue shirt standing in a garden .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000092795.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>Two friends enjoy time spent together .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>10002456.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>Several men in hard hats are operating a gian...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       image_name  ...                                            comment\n",
              "0  1000092795.jpg  ...   Two young guys with shaggy hair look at their...\n",
              "1  1000092795.jpg  ...   Two young , White males are outside near many...\n",
              "2  1000092795.jpg  ...   Two men in green shirts are standing in a yard .\n",
              "3  1000092795.jpg  ...       A man in a blue shirt standing in a garden .\n",
              "4  1000092795.jpg  ...            Two friends enjoy time spent together .\n",
              "5    10002456.jpg  ...   Several men in hard hats are operating a gian...\n",
              "\n",
              "[6 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve62VVO-SrFL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load images\n",
        "# needed transformation: to tensor and normalize\n",
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "input_images = []\n",
        "for filename in glob.glob(\"/content/drive/My Drive/SML/SML_Project/Chunk1/img1_200/*.jpg\"):\n",
        "    im=Image.open(filename)\n",
        "    im=transform(im) # apply transformation while loading\n",
        "    input_images.append(im)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VH9C27vHT9RT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "4ec51412-695e-49bb-8b34-17b8a8ceee05"
      },
      "source": [
        "for i in range(10):\n",
        "  print(input_images[i].shape)\n",
        "# not all images have the same shape but it shouldn't be a problem for ResNet\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 500, 335])\n",
            "torch.Size([3, 375, 500])\n",
            "torch.Size([3, 332, 500])\n",
            "torch.Size([3, 450, 450])\n",
            "torch.Size([3, 338, 450])\n",
            "torch.Size([3, 500, 375])\n",
            "torch.Size([3, 375, 500])\n",
            "torch.Size([3, 374, 500])\n",
            "torch.Size([3, 338, 450])\n",
            "torch.Size([3, 333, 500])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdSOKP_OpM4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim\n",
        "import torch.utils.data\n",
        "from torch import nn\n",
        "import torchvision.transforms as transforms\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "# Model parameters\n",
        "#decoder_dim = \n",
        "dropout = 0.5 # suggested\n",
        "\n",
        "# Parameters\n",
        "epoch = 1000\n",
        "start_epoch = 0\n",
        "epochs_since_improvement = 0 # keeps track of number of epochs since there's been an improvement in validation BLEU\n",
        "batch_size = 32\n",
        "decoder_lr = 1e-4  # learning rate\n",
        "checkpoint = None\n",
        "bleu4 = 0. # bleu score -> https://www.aclweb.org/anthology/P02-1040.pdf\n",
        "workers = 1 # for dataloaders: how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h310E0RxOnt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# maybe this should be saved in a utils.py file\n",
        "\n",
        "def adjust_learning_rate(optimizer, shrink_factor):\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = param_group['lr'] * shrink_factor\n",
        "\n",
        "def clip_gradient(optimizer, grad_clip):\n",
        "  for i in optimizer.param_groups:\n",
        "    for param in group['params']:\n",
        "      if param.grad is not None:\n",
        "        param.grad.data.clamp_(-grad_clip, grad_clip)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkllCpdQvygE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "dedd0105-bd87-4587-9ef5-22ace2d1bbd3"
      },
      "source": [
        "# Vocabulary\n",
        "\n",
        "# Decoder\n",
        "#decoder = RNN(decoder_dim, vocabolary_size, dropout = dropout)\n",
        "\n",
        "decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, \n",
        "                                                   decoder.parameters()),\n",
        "                                     lr=decoder_lr)\n",
        "decoder = decoder.to(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# Epochs\n",
        "for epoch in range(start_epoch, epochs):\n",
        "  # Decay learning rate if there is no improvement for 8 consecutive epochs, and terminate training after 20\n",
        "  if epochs_since_improvement == 20:\n",
        "    break\n",
        "  if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n",
        "    adjust_learning_rate(decoder_optimizer, 0.8)\n",
        "\n",
        "  # Training\n",
        "  train(train_loader=train_loader,\n",
        "        decoder=decoder,\n",
        "        criterion=criterion,\n",
        "        decoder_optimizer=decoder_optimizer,\n",
        "        epoch=epoch)\n",
        "  \n",
        "  # Validation\n",
        "  #bleu4_new\n",
        "\n",
        "  # BLEU4\n",
        "  flag1 = bleu4_new > bleu4\n",
        "  bleu4 = max(bleu4_new, bleu4)\n",
        "  if not flag1:\n",
        "    epochs_since_improvement += 1\n",
        "  else:\n",
        "    epochs_since_improvement = 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b4f7c9b96df8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_atm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI6CWlA8KZcL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "class Splitted_dataset(Dataset):\n",
        "\n",
        "  def __init__(self, csv, folder):\n",
        "    # csv: path of the csv file with captions\n",
        "    # folder: directory with all images\n",
        "    \n",
        "    self.df = pd.read_csv(csv)\n",
        "    self.captions_col = self.df['comment']\n",
        "    self.image_name_col = self.df['image_name']\n",
        "\n",
        "    self.df['length'] = self.captions_col.apply(lambda x: len(x.split()))\n",
        "    self.length_col = self.df['length']\n",
        "\n",
        "    self.folder = folder\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  # not useful atm: def __getitem__(self, image_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4o800gv7QrH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def create (csv, folder):\n",
        "  train_set = Splitted_dataset(csv = csv, \n",
        "                               folder = folder)\n",
        "  return train_set\n",
        "\n",
        "def split(csv, folder, val_size):\n",
        "  '''\n",
        "    @csv: name of the csv\n",
        "    @folder: path to csv\n",
        "  '''\n",
        "  path_to_csv = os.path.join(folder, csv)\n",
        "  output_path_train = os.path.join(folder, \"train.csv\")\n",
        "  output_path_val = os.path.join(folder, \"val.csv\")\n",
        "\n",
        "  df = pd.read_csv(path_to_csv, sep='|')\n",
        "\n",
        "  # we want to split our dataset given itself and the % of sample for validation\n",
        "  num = len(df)\n",
        "  index = list(range(num))\n",
        "  np.random.shuffle(index) # pick at random\n",
        "  flag_split = int(val_size * num)\n",
        "\n",
        "  train_index = index[flag_split:]\n",
        "  validation_index = index[:flag_split]\n",
        "\n",
        "  # https://pytorch.org/docs/stable/data.html -> Samples elements randomly from a given list of indices, without replacement\n",
        "  #train_sampler = SubsetRandomSampler(train_index)\n",
        "  #validation_sampler = SubsetRandomSampler(validation_index)\n",
        "\n",
        "  df_train = df.iloc[train_index]\n",
        "  df_val = df.iloc[validation_index]\n",
        "\n",
        "  df_train.to_csv(output_path_train, index=False)\n",
        "  df_val.to_csv(output_path_val, index=False)\n",
        "\n",
        "  return df_train, df_val\n",
        "\n",
        "def loaders(train_set, train_samplers, validation_sampler, batch_size, val_size, num_workers, csv, folder):\n",
        "\n",
        "  train_sampler, validation_sampler = split(train_set, val_size)\n",
        "  train_loader = DataLoader(train_set,\n",
        "                            batch_size = batch_size,\n",
        "                            sampler = train_sampler,\n",
        "                            num_workers = num_workers)\n",
        "  val_loader = DataLoader(train_set,\n",
        "                          batch_size = batch_size,\n",
        "                          sampler = validation_sampler,\n",
        "                          num_workers = num_workers)\n",
        "  return train_loader, val_loader\n",
        "\n",
        "class imageCaptionDataset(Dataset):\n",
        "\n",
        "  '''\n",
        "    What we want to achieve is a mapping of type:\n",
        "    { img_name: [img_tensor, caption_1, caption_2, caption_3, caption_4, caption_5]}\n",
        "  '''\n",
        "\n",
        "  #TODO: should have a function that puts the two items in the same dictionary\n",
        "  def __init__(self, preProcessedImages, preProcessedCaptions):\n",
        "    '''\n",
        "      Here we should input two dictionaries preProcessedImages and preProcessedCaptions\n",
        "    '''\n",
        "    self.big_Dict = merge_dic(preProcessedImages, preProcessedCaptions)\n",
        "\n",
        "  #TODO: should return the value associated with the given key\n",
        "  def __getitem__(key):\n",
        "    pass\n",
        "\n",
        "  def __merge_dic(d1, d2):\n",
        "   '''\n",
        "     Merge dictionaries and keep values of common keys in list\n",
        "   '''\n",
        "   d3 = {**d1, **d2}\n",
        "   for key, value in d3.items():\n",
        "       if key in d1 and key in d2:\n",
        "               d3[key] = [value , d1[key]]\n",
        "   return d3\n",
        "\n",
        "  #\n",
        "\n",
        "#class imageCaptionSampler(Sampler):\n",
        "  \n",
        " # def __iter__()\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqV8W_VlqUw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_loader, decoder, criterion, decoder_optimizer, epoch):\n",
        "\n",
        "  decoder.train() # train mode\n",
        "\n",
        "  # Batch\n",
        "  for i, (image, caption, captionlen) in enumerate(train_loader):\n",
        "    \n",
        "    # Move to GPU, if available\n",
        "    image = image.to(device)\n",
        "    caption = caption.to(device)\n",
        "    captionlen = captionlen.to(device)\n",
        "\n",
        "    # Forward \n",
        "    #decoder output = decoder(image, caption, captionlen)\n",
        "\n",
        "    # Remove timesteps that we didn't decode at, or are pads\n",
        "    # https://pytorch.org/docs/master/generated/torch.nn.utils.rnn.pack_padded_sequence.html\n",
        "    # https://www.tensorflow.org/tutorials/text/image_captioning\n",
        "    scores, _ = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
        "    targets, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = criterion(scores, targets)\n",
        "\n",
        "    # Back propagation\n",
        "    decoder_optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradients\n",
        "    # https://machinelearningmastery.com/how-to-avoid-exploding-gradients-in-neural-networks-with-gradient-clipping/\n",
        "    if grad_clip is not None:\n",
        "      clip_gradient(decoder_optimizer, grad_clip)\n",
        "\n",
        "    # Update weights\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLvCXBAXryfc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "outputId": "6e35acc2-1948-4ea4-8480-9c2662deb07c"
      },
      "source": [
        "split('results.csv', '/content/drive/My Drive/SML_Project/Chunk1', 0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(            image_name  ...                                            comment\n",
              " 69824   3417672954.jpg  ...                       A skateboarder in the city .\n",
              " 128769  4971283602.jpg  ...              Two students kissing at a crosswalk .\n",
              " 59869   3194533056.jpg  ...                        Women are dancing outside .\n",
              " 18745   2156131463.jpg  ...   People in bathing suits are on a beach lookin...\n",
              " 126399  4934630252.jpg  ...               A person sitting on a hill outside .\n",
              " ...                ...  ...                                                ...\n",
              " 150788  7376855592.jpg  ...     A rodeo player is being taken down by a bull .\n",
              " 72501   3475861944.jpg  ...   A group of people in costumes are gathered , ...\n",
              " 4328     128479788.jpg  ...   Two kids with blond-hair sit atop a four-whee...\n",
              " 19200   2174648405.jpg  ...   Women in dresses work the assembly line while...\n",
              " 53373   3039675864.jpg  ...      A man riding a motorcycle down the sidewalk .\n",
              " \n",
              " [127132 rows x 3 columns],\n",
              "             image_name  ...                                            comment\n",
              " 150485   732780960.jpg  ...   A man wearing khaki shorts and striped t-shir...\n",
              " 81628   3679341667.jpg  ...                         two women are kickboxing .\n",
              " 93292    416902908.jpg  ...   A man is using a broom to sweep the streets i...\n",
              " 88259   3922934080.jpg  ...                 Two kids sitting on the sidewalk .\n",
              " 39243    268044066.jpg  ...   A man touches a tall tree stump while a woman...\n",
              " ...                ...  ...                                                ...\n",
              " 3716    1248734482.jpg  ...   A closeup of a young girl with a pink shirt l...\n",
              " 132394   522712748.jpg  ...           The Ferry Boat sets sail along the bay .\n",
              " 58569   3169213620.jpg  ...        A young child playing with his toy trains .\n",
              " 70978   3441760321.jpg  ...                a man with a guitar waving his hand\n",
              " 150281  7300169048.jpg  ...   Two young girls wearing white tennis uniforms...\n",
              " \n",
              " [31783 rows x 3 columns])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}