{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/earendil94/SMLExam/blob/master/Leo_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLD803bQHlrZ",
        "colab_type": "text"
      },
      "source": [
        "**STATISTICAL MACHINE LEARNING**\n",
        "\n",
        "ARRIGHI Leonardo, BRAND Francesco, DORIGO Claudia\n",
        "\n",
        "\n",
        "Dataset folder is saved in \"/content/drive/My Drive/SML/SML_Project\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADn7VmfFHe_a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "03308378-e302-4330-a4c8-16ef87e9145e"
      },
      "source": [
        "# link colab and drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "# then follow passages"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqQgtn1AuSAi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "214f9db0-b8b6-45b8-f08f-b5c67490085d"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from IPython import display\n",
        "\n",
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "torch.manual_seed(160898)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Device: {}'.format(device))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eycNavNlqLPg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b4ddae88-03ef-445e-908d-8a08f3c06e65"
      },
      "source": [
        "csv = '/content/drive/My Drive/SML/SML_Project/Chunk1/results.csv'\n",
        "imgs = '/content/drive/My\\ Drive/SML/SML_Project/Chunk1/img1_200/'\n",
        "\n",
        "!ls {imgs} | wc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    200     200    2382\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmvRFfu92TsE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "outputId": "b410259e-1356-49a7-8102-b1217c44fc29"
      },
      "source": [
        "# Load csv\n",
        "df = pd.read_csv(csv, delimiter='|') \n",
        "print(df.shape)\n",
        "print(df.columns[2], df.columns[2] == ' comment') \n",
        "df[' comment'].values[0]\n",
        "df.head(6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(158915, 3)\n",
            " comment True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>comment_number</th>\n",
              "      <th>comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000092795.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>Two young guys with shaggy hair look at their...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000092795.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>Two young , White males are outside near many...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000092795.jpg</td>\n",
              "      <td>2</td>\n",
              "      <td>Two men in green shirts are standing in a yard .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000092795.jpg</td>\n",
              "      <td>3</td>\n",
              "      <td>A man in a blue shirt standing in a garden .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000092795.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>Two friends enjoy time spent together .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>10002456.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>Several men in hard hats are operating a gian...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       image_name  ...                                            comment\n",
              "0  1000092795.jpg  ...   Two young guys with shaggy hair look at their...\n",
              "1  1000092795.jpg  ...   Two young , White males are outside near many...\n",
              "2  1000092795.jpg  ...   Two men in green shirts are standing in a yard .\n",
              "3  1000092795.jpg  ...       A man in a blue shirt standing in a garden .\n",
              "4  1000092795.jpg  ...            Two friends enjoy time spent together .\n",
              "5    10002456.jpg  ...   Several men in hard hats are operating a gian...\n",
              "\n",
              "[6 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqLuRn2FGTwT",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing Captions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LANglfWGSis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To put into a module\n",
        "import os\n",
        "from torchtext.data import Field\n",
        "from torchtext.data import TabularDataset\n",
        "from torchtext.data import Iterator\n",
        "\n",
        "def prepare_data(path, input_file, output_file):\n",
        "  input_path = os.path.join(path, input_file)\n",
        "  output_path = os.path.join(path, output_file)\n",
        "  df = pd.read_csv(input_path, sep = \"|\")\n",
        "  captions_array = df[' comment']\n",
        "  captions_array.fillna(\"\", inplace=True)\n",
        "  \n",
        "  image_names = df[\"image_name\"].values\n",
        "  image_number = []\n",
        "  for i in range(0, len(image_names)):\n",
        "    image_number.append(image_names[i].split('.')[0])\n",
        "\n",
        "  df.drop(labels=['image_name', ' comment_number'], axis=1, inplace=True)\n",
        "  df.index = image_number\n",
        "  df.to_csv(output_path, index_label=\"image_number\")\n",
        "  return output_path\n",
        "\n",
        "def build_vocab(path_to_caption_file, caption_file):\n",
        "  output_path = prepare_data(path_to_caption_file, caption_file, \"clean.csv\")\n",
        "  tokenize = lambda x : x.split()\n",
        "  TEXT = Field(sequential = True, tokenize = tokenize, lower=True, init_token='<start>', eos_token='<end>')\n",
        "  LABEL = Field(sequential=False, use_vocab=False)\n",
        "  td_datafields = [(\"image_number\", LABEL ),\n",
        "                  (\"comment\", TEXT)]\n",
        "\n",
        "  trn = TabularDataset(\n",
        "              path=output_path, # the root directory where the data lies\n",
        "              format='csv',\n",
        "              skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
        "              fields=td_datafields,\n",
        "              )\n",
        "  \n",
        "  TEXT.build_vocab(trn)\n",
        "  return TEXT.vocab\n",
        "\n",
        "def word_caption_to_index(path_to_caption_file, caption_file):\n",
        "  '''\n",
        "    This function takes as input the file containing the captions\n",
        "    and returns a matrix of the captions indexed with respect to the inner\n",
        "    vocabulary, as well as an array that can be used to map the indexed caption\n",
        "    to the image it belongs to.\n",
        "    @path_to_caption_file: the path to file containing the captions\n",
        "    @caption_file: the name of the file containing the captions\n",
        "  '''\n",
        "\n",
        "  output_path = prepare_data(path_to_caption_file, caption_file, \"clean.csv\")\n",
        "  tokenize = lambda x : x.split()\n",
        "  TEXT = Field(sequential = True, tokenize = tokenize, lower=True, init_token='<start>', eos_token='<end>')\n",
        "  LABEL = Field(sequential=False, use_vocab=False)\n",
        "\n",
        "  td_datafields = [(\"image_number\", LABEL ),\n",
        "                  (\"comment\", TEXT)]\n",
        "\n",
        "  trn = TabularDataset(\n",
        "                path=output_path, # the root directory where the data lies\n",
        "                format='csv',\n",
        "                skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
        "                fields=td_datafields\n",
        "                )\n",
        "  \n",
        "  TEXT.build_vocab(trn)\n",
        "  train_iter = Iterator(trn, batch_size=len(trn), device = -1)\n",
        "\n",
        "  for i in train_iter: #TODO:I really havent got it how this still works honestly\n",
        "    cmt = i.comment\n",
        "    img = i.image_number\n",
        "\n",
        "  return cmt.T, img\n",
        "\n",
        "def vocab_as_dict(path_to_caption_file, caption_file):\n",
        "  vocab = build_vocab(path_to_caption_file, caption_file)\n",
        "  return vocab.stoi\n",
        "\n",
        "def tensor_to_word(indexed_word, vocab):\n",
        "  for i in indexed_word:\n",
        "    k = i.item()\n",
        "    if k == 1:\n",
        "      break\n",
        "    else:\n",
        "      print(vocab.itos[k], end = \" \")\n",
        "    \n",
        "  \n",
        "\n",
        "def get_caption_from_image(caption_indexes, caption_refs, image_number):\n",
        "  #If image name actually has the .jpg tail\n",
        "  caption_refs = (caption_refs == int(image_number))\n",
        "  caption_refs = caption_refs.nonzero().T.numpy()[0].tolist()\n",
        "  return caption_indexes[caption_refs]\n",
        "\n",
        "def buildCaptionDict(path_to_caption_file, caption_file):\n",
        "\n",
        "  caption_index, refs = word_caption_to_index(path_to_caption_file, caption_file)\n",
        "  refs_list = refs.numpy().tolist()\n",
        "  refs_set = set(refs_list)\n",
        "  unique_refs_list = list(refs_set)\n",
        "\n",
        "  df = []\n",
        "  df = pd.DataFrame(columns=[\"Image_number\", \"Caption_1\", \"Caption_2\", \"Caption_3\", \"Caption_4\", \"Caption_5\"])\n",
        "\n",
        "  for i in range(0, len(unique_refs_list)):\n",
        "    capt_1 = get_caption_from_image(caption_index, refs, unique_refs_list[i])[0]\n",
        "    capt_2 = get_caption_from_image(caption_index, refs, unique_refs_list[i])[1]\n",
        "    capt_3 = get_caption_from_image(caption_index, refs, unique_refs_list[i])[2]\n",
        "    capt_4 = get_caption_from_image(caption_index, refs, unique_refs_list[i])[3]\n",
        "    capt_5 = get_caption_from_image(caption_index, refs, unique_refs_list[i])[4]\n",
        "    df.loc[i] = [unique_refs_list[i], capt_1, capt_2, capt_3, capt_4, capt_5]\n",
        "\n",
        "  df.set_index('Image_number', inplace=True)\n",
        "  capt_dict = df.to_dict('index')\n",
        "\n",
        "  return capt_dict\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve62VVO-SrFL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load images\n",
        "# needed transformation: to tensor and normalize\n",
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "input_images = []\n",
        "for filename in glob.glob(\"/content/drive/My Drive/SML/SML_Project/Chunk1/img1_200/*.jpg\"):\n",
        "    im=Image.open(filename)\n",
        "    im=transform(im) # apply transformation while loading\n",
        "    input_images.append(im)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VH9C27vHT9RT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "4ec51412-695e-49bb-8b34-17b8a8ceee05"
      },
      "source": [
        "for i in range(10):\n",
        "  print(input_images[i].shape)\n",
        "# not all images have the same shape but it shouldn't be a problem for ResNet\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 500, 335])\n",
            "torch.Size([3, 375, 500])\n",
            "torch.Size([3, 332, 500])\n",
            "torch.Size([3, 450, 450])\n",
            "torch.Size([3, 338, 450])\n",
            "torch.Size([3, 500, 375])\n",
            "torch.Size([3, 375, 500])\n",
            "torch.Size([3, 374, 500])\n",
            "torch.Size([3, 338, 450])\n",
            "torch.Size([3, 333, 500])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdSOKP_OpM4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim\n",
        "import torch.utils.data\n",
        "from torch import nn\n",
        "import torchvision.transforms as transforms\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "# Model parameters\n",
        "#decoder_dim = \n",
        "dropout = 0.5 # suggested\n",
        "\n",
        "# Parameters\n",
        "epoch = 1000\n",
        "start_epoch = 0\n",
        "epochs_since_improvement = 0 # keeps track of number of epochs since there's been an improvement in validation BLEU\n",
        "batch_size = 32\n",
        "decoder_lr = 1e-4  # learning rate\n",
        "checkpoint = None\n",
        "bleu4 = 0. # bleu score -> https://www.aclweb.org/anthology/P02-1040.pdf\n",
        "workers = 1 # for dataloaders: how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h310E0RxOnt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# maybe this should be saved in a utils.py file\n",
        "\n",
        "def adjust_learning_rate(optimizer, shrink_factor):\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = param_group['lr'] * shrink_factor\n",
        "\n",
        "def clip_gradient(optimizer, grad_clip):\n",
        "  for i in optimizer.param_groups:\n",
        "    for param in group['params']:\n",
        "      if param.grad is not None:\n",
        "        param.grad.data.clamp_(-grad_clip, grad_clip)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkllCpdQvygE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "dedd0105-bd87-4587-9ef5-22ace2d1bbd3"
      },
      "source": [
        "# Vocabulary\n",
        "\n",
        "# Decoder\n",
        "#decoder = RNN(decoder_dim, vocabolary_size, dropout = dropout)\n",
        "\n",
        "decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, \n",
        "                                                   decoder.parameters()),\n",
        "                                     lr=decoder_lr)\n",
        "decoder = decoder.to(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# Epochs\n",
        "for epoch in range(start_epoch, epochs):\n",
        "  # Decay learning rate if there is no improvement for 8 consecutive epochs, and terminate training after 20\n",
        "  if epochs_since_improvement == 20:\n",
        "    break\n",
        "  if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n",
        "    adjust_learning_rate(decoder_optimizer, 0.8)\n",
        "\n",
        "  # Training\n",
        "  train(train_loader=train_loader,\n",
        "        decoder=decoder,\n",
        "        criterion=criterion,\n",
        "        decoder_optimizer=decoder_optimizer,\n",
        "        epoch=epoch)\n",
        "  \n",
        "  # Validation\n",
        "  #bleu4_new\n",
        "\n",
        "  # BLEU4\n",
        "  flag1 = bleu4_new > bleu4\n",
        "  bleu4 = max(bleu4_new, bleu4)\n",
        "  if not flag1:\n",
        "    epochs_since_improvement += 1\n",
        "  else:\n",
        "    epochs_since_improvement = 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b4f7c9b96df8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_atm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI6CWlA8KZcL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "class Splitted_dataset(Dataset):\n",
        "\n",
        "  def __init__(self, csv, folder):\n",
        "    # csv: path of the csv file with captions\n",
        "    # folder: directory with all images\n",
        "    \n",
        "    self.df = pd.read_csv(csv)\n",
        "    self.captions_col = self.df['comment']\n",
        "    self.image_name_col = self.df['image_name']\n",
        "\n",
        "    self.df['length'] = self.captions_col.apply(lambda x: len(x.split()))\n",
        "    self.length_col = self.df['length']\n",
        "\n",
        "    self.folder = folder\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  # not useful atm: def __getitem__(self, image_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4o800gv7QrH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from torch import tensor\n",
        "\n",
        "\n",
        "def create (csv, folder):\n",
        "  train_set = Splitted_dataset(csv = csv, \n",
        "                               folder = folder)\n",
        "  return train_set\n",
        "\n",
        "def split(csv, folder, val_size):\n",
        "  '''\n",
        "    @csv: name of the csv\n",
        "    @folder: path to csv\n",
        "  '''\n",
        "  path_to_csv = os.path.join(folder, csv)\n",
        "  output_path_train = os.path.join(folder, \"train.csv\")\n",
        "  output_path_val = os.path.join(folder, \"val.csv\")\n",
        "\n",
        "  df = pd.read_csv(path_to_csv, sep='|')\n",
        "\n",
        "  # we want to split our dataset given itself and the % of sample for validation\n",
        "  num = len(df)\n",
        "  index = list(range(num))\n",
        "  np.random.shuffle(index) # pick at random\n",
        "  flag_split = int(val_size * num)\n",
        "\n",
        "  train_index = index[flag_split:]\n",
        "  validation_index = index[:flag_split]\n",
        "\n",
        "  # https://pytorch.org/docs/stable/data.html -> Samples elements randomly from a given list of indices, without replacement\n",
        "  #train_sampler = SubsetRandomSampler(train_index)\n",
        "  #validation_sampler = SubsetRandomSampler(validation_index)\n",
        "\n",
        "  df_train = df.iloc[train_index]\n",
        "  df_val = df.iloc[validation_index]\n",
        "\n",
        "  df_train.to_csv(output_path_train, index=False)\n",
        "  df_val.to_csv(output_path_val, index=False)\n",
        "\n",
        "  return df_train, df_val\n",
        "\n",
        "def loaders(train_set, train_samplers, validation_sampler, batch_size, val_size, num_workers, csv, folder):\n",
        "\n",
        "  train_sampler, validation_sampler = split(train_set, val_size)\n",
        "  train_loader = DataLoader(train_set,\n",
        "                            batch_size = batch_size,\n",
        "                            sampler = train_sampler,\n",
        "                            num_workers = num_workers)\n",
        "  val_loader = DataLoader(train_set,\n",
        "                          batch_size = batch_size,\n",
        "                          sampler = validation_sampler,\n",
        "                          num_workers = num_workers)\n",
        "  return train_loader, val_loader\n",
        "\n",
        "class imageCaptionDataset(Dataset):\n",
        "\n",
        "  '''\n",
        "    What we want to achieve is a mapping of type:\n",
        "    { img_name: [img_tensor, caption_1, caption_2, caption_3, caption_4, caption_5]}\n",
        "  '''\n",
        "\n",
        "  #TODO: should have a function that puts the two items in the same dictionary\n",
        "  def __init__(self, preProcessedImages, preProcessedCaptions):\n",
        "    '''\n",
        "      Here we should input two dictionaries preProcessedImages and preProcessedCaptions\n",
        "    '''\n",
        "    self.big_Dict = self.__merge_dict__(preProcessedImages, preProcessedCaptions)\n",
        "    #self.processed = self.__getitem__(key) #TODO (Brand): I don't quite get this\n",
        "    \n",
        "  #TODO: should return the value associated with the given key\n",
        "  def __getitem__(self, key):\n",
        "    '''\n",
        "      Given dictionary and key, it returns a tuple eith the value of the dictionary (best caption, image) and the length of the best caption \n",
        "    '''\n",
        "    self.big_Dict = self.__one_pick__(self.big_Dict)\n",
        "    val = []\n",
        "    for i in big_Dict.get(key):\n",
        "      val.append(i)\n",
        "    val.append(len(val[0]))\n",
        "    return val\n",
        "\n",
        "  def __merge_dict__(self, d1, d2):\n",
        "   '''\n",
        "     Merge dictionaries and keep values of common keys in list\n",
        "   '''\n",
        "   d3 = {**d1, **d2}\n",
        "   for key, value in d3.items():\n",
        "       if key in d1 and key in d2:\n",
        "               d3[key] = [value , d1[key]]\n",
        "   return d3\n",
        "\n",
        "   # this method will be changed\n",
        "  def __choose_best__(self, d):\n",
        "    '''\n",
        "      Choose a particular caption\n",
        "    '''\n",
        "    val = random.choice(list(d.values()))\n",
        "    return val\n",
        "\n",
        "  # service function\n",
        "  def __one_pick__(self, d):\n",
        "    '''\n",
        "      It returns a dictionary {best caption, image}\n",
        "    '''\n",
        "    tryd = {}\n",
        "    for e in d:\n",
        "      print(e)\n",
        "      tryd[e] = {self.__choose_best__(d[e][0]), d[e][1]}\n",
        "    return tryd\n",
        "\n",
        "  #\n",
        "\n",
        "#class imageCaptionSampler(Sampler):\n",
        "  \n",
        " # def __iter__()\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqV8W_VlqUw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_loader, decoder, criterion, decoder_optimizer, epoch):\n",
        "\n",
        "  decoder.train() # train mode\n",
        "\n",
        "  # Batch\n",
        "  for i, (image, caption, captionlen) in enumerate(train_loader):\n",
        "    \n",
        "    # Move to GPU, if available\n",
        "    image = image.to(device)\n",
        "    caption = caption.to(device)\n",
        "    captionlen = captionlen.to(device)\n",
        "\n",
        "    # Forward \n",
        "    #decoder output = decoder(image, caption, captionlen)\n",
        "\n",
        "    # Remove timesteps that we didn't decode at, or are pads\n",
        "    # https://pytorch.org/docs/master/generated/torch.nn.utils.rnn.pack_padded_sequence.html\n",
        "    # https://www.tensorflow.org/tutorials/text/image_captioning\n",
        "    scores, _ = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
        "    targets, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = criterion(scores, targets)\n",
        "\n",
        "    # Back propagation\n",
        "    decoder_optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradients\n",
        "    # https://machinelearningmastery.com/how-to-avoid-exploding-gradients-in-neural-networks-with-gradient-clipping/\n",
        "    if grad_clip is not None:\n",
        "      clip_gradient(decoder_optimizer, grad_clip)\n",
        "\n",
        "    # Update weights\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLvCXBAXryfc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "outputId": "6e35acc2-1948-4ea4-8480-9c2662deb07c"
      },
      "source": [
        "split('results.csv', '/content/drive/My Drive/SML_Project/Chunk1', 0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(            image_name  ...                                            comment\n",
              " 69824   3417672954.jpg  ...                       A skateboarder in the city .\n",
              " 128769  4971283602.jpg  ...              Two students kissing at a crosswalk .\n",
              " 59869   3194533056.jpg  ...                        Women are dancing outside .\n",
              " 18745   2156131463.jpg  ...   People in bathing suits are on a beach lookin...\n",
              " 126399  4934630252.jpg  ...               A person sitting on a hill outside .\n",
              " ...                ...  ...                                                ...\n",
              " 150788  7376855592.jpg  ...     A rodeo player is being taken down by a bull .\n",
              " 72501   3475861944.jpg  ...   A group of people in costumes are gathered , ...\n",
              " 4328     128479788.jpg  ...   Two kids with blond-hair sit atop a four-whee...\n",
              " 19200   2174648405.jpg  ...   Women in dresses work the assembly line while...\n",
              " 53373   3039675864.jpg  ...      A man riding a motorcycle down the sidewalk .\n",
              " \n",
              " [127132 rows x 3 columns],\n",
              "             image_name  ...                                            comment\n",
              " 150485   732780960.jpg  ...   A man wearing khaki shorts and striped t-shir...\n",
              " 81628   3679341667.jpg  ...                         two women are kickboxing .\n",
              " 93292    416902908.jpg  ...   A man is using a broom to sweep the streets i...\n",
              " 88259   3922934080.jpg  ...                 Two kids sitting on the sidewalk .\n",
              " 39243    268044066.jpg  ...   A man touches a tall tree stump while a woman...\n",
              " ...                ...  ...                                                ...\n",
              " 3716    1248734482.jpg  ...   A closeup of a young girl with a pink shirt l...\n",
              " 132394   522712748.jpg  ...           The Ferry Boat sets sail along the bay .\n",
              " 58569   3169213620.jpg  ...        A young child playing with his toy trains .\n",
              " 70978   3441760321.jpg  ...                a man with a guitar waving his hand\n",
              " 150281  7300169048.jpg  ...   Two young girls wearing white tennis uniforms...\n",
              " \n",
              " [31783 rows x 3 columns])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNKOyAXdEoZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shelve\n",
        "\n",
        "def dict_from_shelve(shelve_file):\n",
        "  dictionary = {}\n",
        "  d=shelve.open(shelve_file)\n",
        "  for k in d.keys():\n",
        "    dictionary[k]=d[k]\n",
        "  return dictionary\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nADvzspHOe3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f2638a38-ca7e-40c2-f13c-553b8b6d49f6"
      },
      "source": [
        "a = dict_from_shelve('/content/drive/My Drive/SML_Project/Chunk1/img_shelve')\n",
        "b = buildCaptionDict('/content/drive/My Drive/SML_Project/Chunk1','results.csv')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZPlIP-rM6yQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "db44454f-8ab2-40cf-b544-86b9b6dc0453"
      },
      "source": [
        "a = {int(k): v for k,v in a.items()} #TODO:I have integers as keys, Cla has strings: This standardizes to int\n",
        "\n",
        "c = imageCaptionDataset(a,b)\n",
        "c.big_Dict[256063]\n",
        "#c.__getitem__(256063) #TODO: still needs to be fixed"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10002456\n",
            "10287332\n",
            "1243756\n",
            "1317156\n",
            "134206\n",
            "1369162\n",
            "2069887\n",
            "2209317\n",
            "256063\n",
            "2868798\n",
            "3367399\n",
            "3494059\n",
            "36979\n",
            "438106\n",
            "4413714\n",
            "490870\n",
            "5060753\n",
            "5566972\n",
            "5570254\n",
            "6100315\n",
            "6261030\n",
            "6331511\n",
            "6335241\n",
            "6678735\n",
            "6734417\n",
            "81641\n",
            "960092\n",
            "10082347\n",
            "10101477\n",
            "148284\n",
            "2317271\n",
            "2760167\n",
            "3012229\n",
            "371897\n",
            "4183120\n",
            "4926723\n",
            "5570219\n",
            "5722658\n",
            "5871970\n",
            "6338733\n",
            "6905083\n",
            "6920532\n",
            "7013217\n",
            "7510394\n",
            "7656601\n",
            "8378599\n",
            "8443156\n",
            "8664922\n",
            "9324151\n",
            "9600569\n",
            "9637989\n",
            "10459869\n",
            "1254659\n",
            "2148982\n",
            "2784746\n",
            "3160699\n",
            "3662865\n",
            "371902\n",
            "3787801\n",
            "3996401\n",
            "4414061\n",
            "4906946\n",
            "5867606\n",
            "6214447\n",
            "6901479\n",
            "7300628\n",
            "7598674\n",
            "7598946\n",
            "10010052\n",
            "205842\n",
            "2689611\n",
            "3219606\n",
            "371903\n",
            "4515460\n",
            "4985704\n",
            "5230968\n",
            "5400154\n",
            "5558592\n",
            "5648321\n",
            "6338704\n",
            "6398988\n",
            "6659698\n",
            "6664030\n",
            "667626\n",
            "675153\n",
            "6827028\n",
            "7015055\n",
            "807129\n",
            "8404753\n",
            "854749\n",
            "8664920\n",
            "8680922\n",
            "10404007\n",
            "1624481\n",
            "353913\n",
            "390369\n",
            "4376178\n",
            "4489731\n",
            "4576671\n",
            "4749855\n",
            "5444724\n",
            "5624522\n",
            "574181\n",
            "5771732\n",
            "5791244\n",
            "5958182\n",
            "7527111\n",
            "8063007\n",
            "8849890\n",
            "8859482\n",
            "9035232\n",
            "9556225\n",
            "9950858\n",
            "10188041\n",
            "10350842\n",
            "178045\n",
            "2192573\n",
            "3043766\n",
            "3537322\n",
            "3680138\n",
            "4429660\n",
            "5629300\n",
            "5791568\n",
            "5914327\n",
            "5918675\n",
            "6054169\n",
            "6155176\n",
            "6398924\n",
            "65567\n",
            "7520731\n",
            "764507\n",
            "7808046\n",
            "8601145\n",
            "8757787\n",
            "8832804\n",
            "8876934\n",
            "8990627\n",
            "9426826\n",
            "984950\n",
            "10082348\n",
            "1283466\n",
            "1440465\n",
            "1989609\n",
            "2209751\n",
            "2656351\n",
            "301246\n",
            "3025093\n",
            "4162702\n",
            "4199555\n",
            "4386588\n",
            "5088155\n",
            "5333578\n",
            "5377361\n",
            "5402085\n",
            "5729814\n",
            "5858851\n",
            "5919020\n",
            "6339096\n",
            "6398952\n",
            "726414\n",
            "7300624\n",
            "7340189\n",
            "7520721\n",
            "8206921\n",
            "8684718\n",
            "9268793\n",
            "9726060\n",
            "9950913\n",
            "10090841\n",
            "10160966\n",
            "1920465\n",
            "2285664\n",
            "2806447\n",
            "3001353\n",
            "3035057\n",
            "3637013\n",
            "3734864\n",
            "3753939\n",
            "3765589\n",
            "4135695\n",
            "4280272\n",
            "4307968\n",
            "4378823\n",
            "5104045\n",
            "5217116\n",
            "5287405\n",
            "5521996\n",
            "5526034\n",
            "5733760\n",
            "5791070\n",
            "5918840\n",
            "6696219\n",
            "6901333\n",
            "6901412\n",
            "7188003\n",
            "793558\n",
            "8029536\n",
            "8454235\n",
            "881336\n",
            "9556226\n",
            "4564320256\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-223e79845f3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimageCaptionDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#c.big_Dict[256063]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256063\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-69-e6ecbe50a723>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     81\u001b[0m       \u001b[0mGiven\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtuple\u001b[0m \u001b[0meith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbest\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlength\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbest\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     '''\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbig_Dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__one_pick__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbig_Dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbig_Dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-69-e6ecbe50a723>\u001b[0m in \u001b[0;36m__one_pick__\u001b[0;34m(self, d)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m       \u001b[0mtryd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__choose_best__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtryd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2DFAr0jM76N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}