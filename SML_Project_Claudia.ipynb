{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SML_Project_Claudia.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "16af402f76374ede8a3a3c5ef71eb5c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b29d3acdf76249de88dceaf020d8f2c0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0763446d42d644059185fb7704a1e02f",
              "IPY_MODEL_738057c1f1d44fcda61c2f0f433eb25d"
            ]
          }
        },
        "b29d3acdf76249de88dceaf020d8f2c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0763446d42d644059185fb7704a1e02f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_33f3dc0831b543f3aa6bd288c5956fd6",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 241530880,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 241530880,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3d6d13f5acc5484b80d7d39c55833b37"
          }
        },
        "738057c1f1d44fcda61c2f0f433eb25d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_24704cd3816e42c2b10a33c26c7c275c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 230M/230M [00:01&lt;00:00, 124MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d1f557b7dc244cd4b53586d689bae88d"
          }
        },
        "33f3dc0831b543f3aa6bd288c5956fd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3d6d13f5acc5484b80d7d39c55833b37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "24704cd3816e42c2b10a33c26c7c275c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d1f557b7dc244cd4b53586d689bae88d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/earendil94/SMLExam/blob/master/SML_Project_Claudia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLD803bQHlrZ",
        "colab_type": "text"
      },
      "source": [
        "**STATISTICAL MACHINE LEARNING**\n",
        "\n",
        "ARRIGHI Leonardo, BRAND Francesco, DORIGO Claudia\n",
        "\n",
        "\n",
        "Dataset folder is saved in \"/content/drive/My Drive/SML/SML_Project\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xveRq3asNPfy",
        "colab_type": "text"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADn7VmfFHe_a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "c15e734f-556d-4a2e-8cc9-459c438f9c3d"
      },
      "source": [
        "# link colab and drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "# then follow passages"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqQgtn1AuSAi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7d37ba65-44f9-470b-8a95-6cf298fd4e62"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import os\n",
        "import torchvision\n",
        "from torchvision import transforms, models\n",
        "from IPython import display\n",
        "import shelve\n",
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "torch.manual_seed(160898)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Device: {}'.format(device))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7C8-C8vNVN7",
        "colab_type": "text"
      },
      "source": [
        "# Images Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNq2wrM91FDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# APPLY TRANSFORMATIONS TO PIL IMAGE \n",
        "def transform_image(image):\n",
        "  transform = transforms.Compose([transforms.Resize(256),\n",
        "                                  transforms.CenterCrop(224),\n",
        "                                  transforms.ToTensor(),\n",
        "                                  transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "                                  ])\n",
        "  return transform(image)\n",
        "\n",
        "img_prova = Image.open('/content/drive/My Drive/SML/SML_Project/Chunk1/img1_200/1624481.jpg')\n",
        "i = transform_image(img_prova)\n",
        "\n",
        "# PRINT IMAGE HAVING AN IMAGE KEY\n",
        "def print_image_from_key(image_key):\n",
        "  path = '/content/drive/My Drive/SML/SML_Project/All_images/'\n",
        "  im = Image.open(os.path.join(path,str(image_key)+'.jpg'))\n",
        "  %matplotlib inline\n",
        "  imshow(np.asarray(im))\n",
        "\n",
        "#print_image_from_key(1624481)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttU_dLUsvIEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FUNCTION TO BUILD THE DICTIONARY (JUST FIRST 15K IMAGES)\n",
        "# don't run, it takes 2 hours\n",
        "\n",
        "def build_img_shelve(img_folder,shelve_path):\n",
        "  path = os.path.join(img_folder,'*.jpg')\n",
        "  with shelve.open(shelve_path) as d:\n",
        "    for i in glob.glob(path)[:15000]: #just first 15000 otherwise too big\n",
        "      im=Image.open(i)\n",
        "      d[i[len(img_folder):].split('.')[0]]=transform_image(im)\n",
        "\n",
        "    \n",
        "imgs = '/content/drive/My Drive/SML/SML_Project/All_images/'\n",
        "#build_img_shelve(imgs,'/content/drive/My Drive/SML/SML_Project/img_shelve_15k')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lfa2fpOVJjuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DICTIONARY FROM SHELVE\n",
        "def dict_from_shelve(shelve_file):\n",
        "  dictionary = {}\n",
        "  d=shelve.open(shelve_file)\n",
        "  for k in d.keys():\n",
        "    dictionary[int(k)]=d[k]\n",
        "  return dictionary\n",
        "\n",
        "chunk1_dict = dict_from_shelve('/content/drive/My Drive/SML/SML_Project/Chunk1/img_shelve')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02vLJcXdNZph",
        "colab_type": "text"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQiPGK8Kxtl8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "e9de4fbf-0775-4016-aef2-ede24278c1df"
      },
      "source": [
        "#https://towardsdatascience.com/automatic-image-captioning-with-cnn-rnn-aae3cd442d83\n",
        "# ENCODER CNN\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        resnet = models.resnet152(pretrained=True)\n",
        "        for param in resnet.parameters():\n",
        "            param.requires_grad_(False)\n",
        "        \n",
        "        modules = list(resnet.children())[:-1] # remove last layer\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.resnet(images)\n",
        "        features = features.view(features.size(0), -1)\n",
        "        features = self.embed(features)\n",
        "        return features\n",
        "\n",
        "'''\n",
        "image = a[10002456]\n",
        "image = image.unsqueeze(0)\n",
        "cnn = EncoderCNN()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  image = image.to('cuda')\n",
        "  cnn.to('cuda')\n",
        "\n",
        "output = cnn(image)\n",
        "print(output,output.shape)\n",
        "'''"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nimage = a[10002456]\\nimage = image.unsqueeze(0)\\ncnn = EncoderCNN()\\n\\nif torch.cuda.is_available():\\n  image = image.to('cuda')\\n  cnn.to('cuda')\\n\\noutput = cnn(image)\\nprint(output,output.shape)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu65feqmxEBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FUNCTION TO CHECK THE RESULT OF CNN \n",
        "#(without removing last layer print 10 more probable classes) \n",
        "import json\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "def CNN_classification(image,model):\n",
        "  path = '/content/drive/My Drive/SML/SML_Project/All_images/'\n",
        "  im = Image.open(os.path.join(path,str(image)+'.jpg'))\n",
        "  model.eval()\n",
        "\n",
        "  out = model(transform_image(im).unsqueeze(0))\n",
        "  #class_probs = torch.nn.functional.softmax(out, dim=0) #not needed\n",
        "\n",
        "  class_idx = json.load(open(\"/content/drive/My Drive/SML/SML_Project/classes.json\"))\n",
        "  idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]\n",
        "\n",
        "  # sort wrt probability and take 10 more probable indexes\n",
        "  for idx in out[0].sort()[1][-10:]:\n",
        "    print(idx2label[idx])\n",
        "\n",
        "  #print the image\n",
        "  %matplotlib inline\n",
        "  imshow(np.asarray(im))\n",
        "\n",
        "#pretrained_ResNet = torch.hub.load('pytorch/vision:v0.6.0', 'resnet152', pretrained=True)\n",
        "#CNN_classification('1624481',pretrained_ResNet)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-6WEgihNdtg",
        "colab_type": "text"
      },
      "source": [
        "# Dataset, sampler, split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fL25f6WEjfU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DATASET,SAMPLER CLASSES (FOR DATALOADER) AND SPLIT METHODS\n",
        "import torch\n",
        "from torch.utils.data import Dataset, Sampler, DataLoader, SubsetRandomSampler\n",
        "import pandas as pd\n",
        "\n",
        "class imageCaptionDataset(Dataset):\n",
        "  '''\n",
        "    What we want to achieve is a mapping of type:\n",
        "    { img_name: [img_tensor, caption_1, caption_2, caption_3, caption_4, caption_5]}\n",
        "  '''\n",
        "\n",
        "  #TODO: should have a function that puts the two items in the same dictionary\n",
        "  def __init__(self, preProcessedImages, preProcessedCaptions):\n",
        "    '''\n",
        "      This function takes in input two dictionaries and merge them in a big_dictionary\n",
        "      according to common key values\n",
        "    '''\n",
        "    self.big_Dict = {}\n",
        "    for k in preProcessedCaptions.keys():\n",
        "      a = preProcessedImages.get(k) # return none if it doesn't exist\n",
        "      if a is not None:\n",
        "        self.big_Dict[k] = [a,preProcessedCaptions[k]]\n",
        "\n",
        "  def __getitem__(self, key):\n",
        "    '''\n",
        "      This function returns the preprocessed image and the first preprocessed comment \n",
        "      associated with the given key\n",
        "      TO DO:\n",
        "      - introduce a function to select the best comment\n",
        "      - define the behaviour when a key is not present\n",
        "    '''\n",
        "    #which_comment = 0 # get first comment - we can add here a function to select the best\n",
        "    comments = self.big_Dict[key][1]\n",
        "    returned_comment = comments.get('Caption_1')\n",
        "    return self.big_Dict[key][0], returned_comment, returned_comment.size()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.big_Dict)\n",
        "\n",
        "  def get_keys(self):\n",
        "    '''\n",
        "      I need this function in the sampler. It returns a list with all the \n",
        "      keys present in the big dictionary\n",
        "    '''\n",
        "    return self.big_Dict.keys()\n",
        "\n",
        "\n",
        "class imageCaptionSampler(Sampler):\n",
        "  '''\n",
        "    TO DO: \n",
        "    - _iter_ which returns an iterable over the dataset\n",
        "    - _len_ which returns the length of the dataset (needed to compute number of batches in dataloader)\n",
        "  '''\n",
        "  def __init__(self, data_source):\n",
        "    self.data_source = data_source\n",
        "\n",
        "  def __iter__(self):\n",
        "    '''\n",
        "      we don't care the order in which iterate the dataset so this function defines\n",
        "      an iterator over the key list\n",
        "    '''\n",
        "    return iter(self.data_source.get_keys())\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data_source.get_keys())  \n",
        "\n",
        "\n",
        "\n",
        "def split(imgDataset, val_size):\n",
        "  '''\n",
        "    @imgDataset: an image caption dataset object\n",
        "    @val_size: percentage of the dataset that should compose the validation set\n",
        "\n",
        "    This function allows us to split our dataset into \n",
        "    a validation set and a training set. This is used internally in \n",
        "    Loader, should check that one.\n",
        "  '''\n",
        "\n",
        "  # We want to split our dataset given itself and the % of sample for validation\n",
        "  num = len(imgDataset)\n",
        "  index = list(imgDataset.get_keys())\n",
        "  np.random.shuffle(index) # pick at random\n",
        "  flag_split = int(val_size * num)\n",
        "\n",
        "  train_index = index[flag_split:]\n",
        "  validation_index = index[:flag_split]\n",
        "\n",
        "  # https://pytorch.org/docs/stable/data.html -> Samples elements randomly from a given list of indices, without replacement\n",
        "  train_sampler = SubsetRandomSampler(train_index)\n",
        "  validation_sampler = SubsetRandomSampler(validation_index)\n",
        "\n",
        "  return train_sampler, validation_sampler\n",
        "\n",
        "def loaders(dataset, val_size, batch_size, num_workers):\n",
        "  ''' \n",
        "    @dataset: an image caption dataset object\n",
        "    @val_size: the percentage (must be [0,1]) of the validation set data\n",
        "    @batch_size: the number of data in each batch\n",
        "    @num_workers: number of subprocesses to use in the data loader\n",
        "  '''\n",
        "\n",
        "  train_sampler, validation_sampler = split(dataset, val_size)\n",
        "  train_loader = DataLoader(dataset,\n",
        "                            batch_size = batch_size,\n",
        "                            sampler = train_sampler,\n",
        "                            num_workers = num_workers)\n",
        "  val_loader = DataLoader(dataset,\n",
        "                          batch_size = batch_size,\n",
        "                          sampler = validation_sampler,\n",
        "                          num_workers = num_workers)\n",
        "  return train_loader, val_loader"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZbGZK0kNie_",
        "colab_type": "text"
      },
      "source": [
        "# Text preProcessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXwd8gvh4GyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CAPTION PREPROCESSING FUNCTIONS\n",
        "from torchtext.data import Field\n",
        "from torchtext.data import TabularDataset\n",
        "from torchtext.data import Iterator\n",
        "\n",
        "def prepare_data(path, input_file, output_file):\n",
        "  input_path = os.path.join(path, input_file)\n",
        "  output_path = os.path.join(path, output_file)\n",
        "  df = pd.read_csv(input_path, sep = \"|\")\n",
        "  captions_array = df[' comment']\n",
        "  captions_array.fillna(\"\", inplace=True)\n",
        "  \n",
        "  image_names = df[\"image_name\"].values\n",
        "  image_number = []\n",
        "  for i in range(0, len(image_names)):\n",
        "    image_number.append(image_names[i].split('.')[0])\n",
        "\n",
        "  df.drop(labels=['image_name', ' comment_number'], axis=1, inplace=True)\n",
        "  df.index = image_number\n",
        "  df.to_csv(output_path, index_label=\"image_number\")\n",
        "  return output_path\n",
        "\n",
        "def build_vocab(path_to_caption_file, caption_file):\n",
        "  output_path = prepare_data(path_to_caption_file, caption_file, \"clean.csv\")\n",
        "  tokenize = lambda x : x.split()\n",
        "  TEXT = Field(sequential = True, tokenize = tokenize, lower=True, init_token='<start>', eos_token='<end>')\n",
        "  LABEL = Field(sequential=False, use_vocab=False)\n",
        "  td_datafields = [(\"image_number\", LABEL ),\n",
        "                  (\"comment\", TEXT)]\n",
        "\n",
        "  trn = TabularDataset(\n",
        "              path=output_path, # the root directory where the data lies\n",
        "              format='csv',\n",
        "              skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
        "              fields=td_datafields,\n",
        "              )\n",
        "  \n",
        "  TEXT.build_vocab(trn)\n",
        "  return TEXT.vocab\n",
        "\n",
        "def word_caption_to_index(path_to_caption_file, caption_file):\n",
        "  '''\n",
        "    This function takes as input the file containing the captions\n",
        "    and returns a matrix of the captions indexed with respect to the inner\n",
        "    vocabulary, as well as an array that can be used to map the indexed caption\n",
        "    to the image it belongs to.\n",
        "    @path_to_caption_file: the path to file containing the captions\n",
        "    @caption_file: the name of the file containing the captions\n",
        "  '''\n",
        "\n",
        "  output_path = prepare_data(path_to_caption_file, caption_file, \"clean.csv\")\n",
        "  tokenize = lambda x : x.split()\n",
        "  TEXT = Field(sequential = True, tokenize = tokenize, lower=True, init_token='<start>', eos_token='<end>')\n",
        "  LABEL = Field(sequential=False, use_vocab=False)\n",
        "\n",
        "  td_datafields = [(\"image_number\", LABEL ),\n",
        "                  (\"comment\", TEXT)]\n",
        "\n",
        "  trn = TabularDataset(\n",
        "                path=output_path, # the root directory where the data lies\n",
        "                format='csv',\n",
        "                skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
        "                fields=td_datafields\n",
        "                )\n",
        "  \n",
        "  TEXT.build_vocab(trn)\n",
        "  train_iter = Iterator(trn, batch_size=len(trn), device = -1)\n",
        "\n",
        "  for i in train_iter: #TODO:I really havent got it how this still works honestly\n",
        "    cmt = i.comment\n",
        "    img = i.image_number\n",
        "\n",
        "  return cmt.T, img\n",
        "\n",
        "def vocab_as_dict(path_to_caption_file, caption_file):\n",
        "  vocab = build_vocab(path_to_caption_file, caption_file)\n",
        "  return vocab.stoi\n",
        "\n",
        "def tensor_to_word(indexed_word, vocab):\n",
        "  for i in indexed_word:\n",
        "    k = i.item()\n",
        "    if k == 1:\n",
        "      break\n",
        "    else:\n",
        "      print(vocab.itos[k], end = \" \")\n",
        "    \n",
        "  \n",
        "\n",
        "def get_caption_from_image(caption_indexes, caption_refs, image_number):\n",
        "  #If image name actually has the .jpg tail\n",
        "  caption_refs = (caption_refs == int(image_number))\n",
        "  caption_refs = caption_refs.nonzero().T.numpy()[0].tolist()\n",
        "  return caption_indexes[caption_refs]\n",
        "\n",
        "def buildCaptionDict(path_to_caption_file, caption_file):\n",
        "\n",
        "  caption_index, refs = word_caption_to_index(path_to_caption_file, caption_file)\n",
        "  refs_list = refs.numpy().tolist()\n",
        "  refs_set = set(refs_list)\n",
        "  unique_refs_list = list(refs_set)\n",
        "\n",
        "  df = []\n",
        "  df = pd.DataFrame(columns=[\"Image_number\", \"Caption_1\", \"Caption_2\", \"Caption_3\", \"Caption_4\", \"Caption_5\"])\n",
        "\n",
        "  for i in range(0, len(unique_refs_list)):\n",
        "    capt_1 = get_caption_from_image(caption_index, refs, unique_refs_list[i])[0]\n",
        "    capt_2 = get_caption_from_image(caption_index, refs, unique_refs_list[i])[1]\n",
        "    capt_3 = get_caption_from_image(caption_index, refs, unique_refs_list[i])[2]\n",
        "    capt_4 = get_caption_from_image(caption_index, refs, unique_refs_list[i])[3]\n",
        "    capt_5 = get_caption_from_image(caption_index, refs, unique_refs_list[i])[4]\n",
        "    df.loc[i] = [unique_refs_list[i], capt_1, capt_2, capt_3, capt_4, capt_5]\n",
        "\n",
        "  df.set_index('Image_number', inplace=True)\n",
        "  capt_dict = df.to_dict('index')\n",
        "\n",
        "  return capt_dict"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goy59SPENnnI",
        "colab_type": "text"
      },
      "source": [
        "# Building dictionary and dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syUMQ1Hs293g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "414e7803-7607-4d1a-82bf-503f66d6993a"
      },
      "source": [
        "# TEST ON OUR DATA\n",
        "# image dictionary\n",
        "chunk1_dict = dict_from_shelve('/content/drive/My Drive/SML/SML_Project/Chunk1/img_shelve')\n",
        "\n",
        "# caption dictionary\n",
        "path_to_caption_file = \"/content/drive/My Drive/SML/SML_Project/Chunk1\"\n",
        "caption_file = \"results.csv\"\n",
        "caption_dict = buildCaptionDict(path_to_caption_file, caption_file)\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loHiiy6k7tFO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "outputId": "a4251450-d2ca-4aed-ba4c-85f7b74a8398"
      },
      "source": [
        "dataset = imageCaptionDataset(chunk1_dict,caption_dict)\n",
        "#chunk1_dict['1624481']\n",
        "img_tensor, caption_tensor, size = dataset[1624481]\n",
        "print(img_tensor)\n",
        "print(caption_tensor)\n",
        "print(size)\n",
        "dataset.get_keys()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-0.7822, -0.6794, -0.6109,  ...,  0.2453,  0.4508,  0.8276],\n",
            "         [-0.8678, -0.8507, -0.8678,  ..., -0.6794, -0.5596, -0.4226],\n",
            "         [-0.8849, -0.8507, -0.8507,  ..., -0.7479, -0.7308, -0.7650],\n",
            "         ...,\n",
            "         [ 0.8447,  0.8447,  0.8276,  ...,  0.3309,  0.3481,  0.3481],\n",
            "         [ 0.9303,  0.9474,  0.9303,  ...,  0.3652,  0.3652,  0.3652],\n",
            "         [ 0.9132,  0.9303,  0.9303,  ...,  0.4166,  0.3823,  0.3652]],\n",
            "\n",
            "        [[-0.6352, -0.4951, -0.3901,  ...,  0.4678,  0.6779,  1.0455],\n",
            "         [-0.7227, -0.6702, -0.6527,  ..., -0.5476, -0.4251, -0.2675],\n",
            "         [-0.7402, -0.6702, -0.6527,  ..., -0.7402, -0.7227, -0.7227],\n",
            "         ...,\n",
            "         [ 1.0980,  1.0980,  1.0805,  ...,  0.5728,  0.5903,  0.5903],\n",
            "         [ 1.1856,  1.2031,  1.1856,  ...,  0.6078,  0.6078,  0.6078],\n",
            "         [ 1.1681,  1.1856,  1.1856,  ...,  0.6604,  0.6254,  0.6078]],\n",
            "\n",
            "        [[-0.6367, -0.5321, -0.4798,  ...,  0.2522,  0.4614,  0.8622],\n",
            "         [-0.7064, -0.7064, -0.6890,  ..., -0.6890, -0.5670, -0.4450],\n",
            "         [-0.7064, -0.6890, -0.6193,  ..., -0.7936, -0.7936, -0.8110],\n",
            "         ...,\n",
            "         [ 1.2457,  1.2457,  1.2282,  ...,  0.7228,  0.7402,  0.7402],\n",
            "         [ 1.3328,  1.3502,  1.3328,  ...,  0.7576,  0.7576,  0.7576],\n",
            "         [ 1.3154,  1.3328,  1.3328,  ...,  0.8099,  0.7751,  0.7576]]])\n",
            "tensor([    2,  1179,  1919,     6,    66,  2669,  7255,   322,   139,  4556,\n",
            "           64,     7, 10034,  5010,     5,     3,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1])\n",
            "torch.Size([84])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys([7340189, 134206, 5377361, 5771732, 4985704, 4199555, 1317156, 793558, 2760167, 10101477, 3680138, 3025093, 6827028, 6696219, 5648321, 667626, 8664920, 8664922, 3160699, 5914327, 8404753, 5521996, 148284, 9324151, 675153, 3035057, 5918675, 5918840, 807129, 5919020, 5526034, 1989609, 5791070, 5791244, 5791568, 6054169, 5400154, 9726060, 5402085, 3043766, 8680922, 4749855, 2784746, 9600569, 4489731, 8684718, 8029536, 2656351, 36979, 301246, 7510394, 6331511, 10002456, 960092, 6335241, 5287405, 438106, 178045, 6338704, 6338733, 6339096, 10010052, 10404007, 7520721, 7520731, 6734417, 574181, 4376178, 8832804, 1624481, 2148982, 4378823, 5558592, 2806447, 6214447, 7656601, 8443156, 7527111, 4906946, 2285664, 1369162, 4515460, 5958182, 4386588, 5566972, 10287332, 1243756, 3996401, 3734864, 5570219, 5570254, 65567, 8454235, 7013217, 984950, 8063007, 8849890, 2689611, 7015055, 854749, 10160966, 9637989, 5444724, 6100315, 726414, 4135695, 3219606, 205842, 7808046, 1254659, 4926723, 8859482, 8990627, 5060753, 8206921, 8601145, 81641, 3753939, 1920465, 6901333, 6901412, 6901479, 4280272, 3494059, 5722658, 4413714, 4414061, 2317271, 6905083, 3367399, 5333578, 7300624, 7300628, 5858851, 353913, 5729814, 9268793, 881336, 8876934, 2192573, 10188041, 3765589, 490870, 5733760, 3637013, 4162702, 5867606, 6261030, 2069887, 1283466, 4429660, 6920532, 5871970, 10459869, 5217116, 6659698, 8757787, 5088155, 6398924, 6398952, 6398988, 764507, 371897, 371903, 371902, 7188003, 6664030, 2209317, 2209751, 4307968, 2868798, 3001353, 3787801, 9556225, 9556226, 5230968, 5624522, 4183120, 4576671, 9950858, 9950913, 9426826, 10082347, 10082348, 8378599, 9035232, 5104045, 3662865, 5629300, 256063, 6678735, 6155176, 10350842, 7598674, 7598946, 390369, 3012229, 10090841, 3537322, 1440465])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF6sHm5DNr--",
        "colab_type": "text"
      },
      "source": [
        "# Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_by1X2Gjxnk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO: Right now this is basically the same exact class described in the towards data science article\n",
        "#Should/Do we have to make any changes to this?\n",
        "class DecoderRNN(nn.Module): \n",
        "  def __init__(self, embed_size, hidden_size, vocab_size, num_layers = 1): #Here we define the layers\n",
        "    super().__init__()\n",
        "    self.embedding_captions_layer = nn.Embedding(vocab_size, embed_size)\n",
        "    self.LSTM = nn.LSTM(input_size = embed_size, hidden_size = hidden_size, \n",
        "                        num_layers = num_layers, batch_first = True)\n",
        "    self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "  def forward(self, features, captions): #Notice that features will only be used when we will have the encoder images in input\n",
        "    captions = captions[:, :-1]\n",
        "    # The following line retrieves the embedded representation of the indexes that we pass to it\n",
        "    embed = self.embedding_captions_layer(captions)\n",
        "    #A couple of comments here: torch.cat concatenates just like in unix\n",
        "    #torch.unsqueeze instead transforms the tensor into a column vector (column since we specify 1 here)\n",
        "    embed = torch.cat((features.unsqueeze(1), embed), dim = 1) \n",
        "    lstm_outputs, _ = self.LSTM(embed)\n",
        "    out = self.linear(lstm_outputs)\n",
        "    return out\n",
        "\n",
        "  def sample(self, inputs, states=None, max_len=20):\n",
        "    output_sentence = []\n",
        "\n",
        "    for i in range(max_len):\n",
        "      lstm_outputs, states = self.LSTM(inputs, states)\n",
        "      lstm_outputs = lstm_outputs.squeeze(1)\n",
        "      out = self.linear(lstm_outputs)\n",
        "      last_pick = out.max(1)[1]\n",
        "      output_sentence.append(last_pick.item())\n",
        "      inputs = self.embedding_captions_layer(last_pick).unsqueeze(1)\n",
        "      \n",
        "    return output_sentence"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ybdPYLQNuWv",
        "colab_type": "text"
      },
      "source": [
        "# Parameters definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjvEY6Ip1eIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#PARAMETERS DEFINITION\n",
        "\n",
        "\n",
        "#from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "# RNN - Model parameters\n",
        "hidden_size = 1000\n",
        "embed_size = 400\n",
        "vocab_size = len(vocab_as_dict('/content/drive/My Drive/SML/SML_Project/Chunk1/', 'results.csv'))\n",
        "dropout = 0.5 # suggested\n",
        "\n",
        "# Parameters\n",
        "epoch = 20\n",
        "start_epoch = 0\n",
        "epochs_since_improvement = 0 # keeps track of number of epochs since there's been an improvement in validation BLEU\n",
        "batch_size = 64\n",
        "decoder_lr = 5e-4  # learning rate\n",
        "checkpoint = None\n",
        "bleu4 = 0. # bleu score -> https://www.aclweb.org/anthology/P02-1040.pdf\n",
        "workers = 1 # for dataloaders: how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process\n",
        "grad_clip = 5. # must be changed after all "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehazjG4c1mN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# maybe this should be saved in a utils.py file\n",
        "\n",
        "def adjust_learning_rate(optimizer, shrink_factor):\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = param_group['lr'] * shrink_factor\n",
        "\n",
        "def clip_gradient(optimizer, grad_clip):\n",
        "  for group in optimizer.param_groups:\n",
        "    for param in group['params']:\n",
        "      if param.grad is not None:\n",
        "        param.grad.data.clamp_(-grad_clip, grad_clip)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp_3FERhNzWc",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifWMIAWg2OgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_loader, encoder, decoder, criterion, decoder_optimizer, epoch, device, grad_clip):\n",
        "\n",
        "  decoder.train() # train mode\n",
        "\n",
        "  # Epochs\n",
        "  for i in range(epoch):\n",
        "    \n",
        "    # Batch\n",
        "    for i_step, (image, caption, captionlen) in enumerate(train_loader):\n",
        "      \n",
        "      # Move to GPU, if available\n",
        "      image = image.to(device)\n",
        "      caption = caption.to(device)\n",
        "      #captionlen = captionlen.to(device)\n",
        "\n",
        "      # Output encoder\n",
        "      image = encoder(image)\n",
        "\n",
        "      # Forward \n",
        "      decoder_output = decoder(image, caption)\n",
        "      \n",
        "      \n",
        "      # Calculate loss\n",
        "      loss = criterion(decoder_output.view(-1, vocab_size), caption.view(-1))\n",
        "      #loss = criterion(decoder_output, caption)\n",
        "      #print('Loss = ', loss)\n",
        "      print_every = 1\n",
        "      stats = 'Epoch [%d/%d], Step: %d, Loss: %.4f, Perplexity: %5.4f' % (i, epoch, i_step, loss.item(), np.exp(loss.item()))\n",
        "      if i_step % print_every == 0:\n",
        "          print('\\r' + stats)\n",
        "    \n",
        "      # Back propagation\n",
        "      decoder_optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      \n",
        "\n",
        "      # Clip gradients\n",
        "      # https://machinelearningmastery.com/how-to-avoid-exploding-gradients-in-neural-networks-with-gradient-clipping/\n",
        "      if grad_clip is not None:\n",
        "        clip_gradient(decoder_optimizer, grad_clip)\n",
        "\n",
        "      # Update weights\n",
        "      decoder_optimizer.step()\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEhmj-8PN1sj",
        "colab_type": "text"
      },
      "source": [
        "# Variable declaration and training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aITyb41r1oMG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540,
          "referenced_widgets": [
            "16af402f76374ede8a3a3c5ef71eb5c8",
            "b29d3acdf76249de88dceaf020d8f2c0",
            "0763446d42d644059185fb7704a1e02f",
            "738057c1f1d44fcda61c2f0f433eb25d",
            "33f3dc0831b543f3aa6bd288c5956fd6",
            "3d6d13f5acc5484b80d7d39c55833b37",
            "24704cd3816e42c2b10a33c26c7c275c",
            "d1f557b7dc244cd4b53586d689bae88d"
          ]
        },
        "outputId": "9e66579c-883b-4b00-c224-f2ff2cff284e"
      },
      "source": [
        "# Encoder\n",
        "encoder = EncoderCNN(embed_size)\n",
        "\n",
        "# Decoder\n",
        "decoder = DecoderRNN(embed_size, hidden_size, vocab_size) #dropout = dropout)\n",
        "\n",
        "decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, \n",
        "                                                   decoder.parameters()),\n",
        "                                     lr=decoder_lr)\n",
        "\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# DataLoader\n",
        "train_loader, val_loader = loaders(dataset, 0.2, batch_size, 1)\n",
        "\n",
        "'''\n",
        "# Epochs\n",
        "for epoch in range(start_epoch, epochs):\n",
        "  # Decay learning rate if there is no improvement for 8 consecutive epochs, and terminate training after 20\n",
        "  if epochs_since_improvement == 20:\n",
        "    break\n",
        "  if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n",
        "    adjust_learning_rate(decoder_optimizer, 0.8)\n",
        "'''\n",
        "\n",
        "# Training\n",
        "train(train_loader=train_loader,\n",
        "      encoder=encoder,\n",
        "      decoder=decoder,\n",
        "      criterion=criterion,\n",
        "      decoder_optimizer=decoder_optimizer,\n",
        "      epoch=epoch,\n",
        "      device=device,\n",
        "      grad_clip=grad_clip)\n",
        "\n",
        "  # Validation\n",
        "  #bleu4_new\n",
        "\n",
        "'''\n",
        "  # BLEU4\n",
        "  flag1 = bleu4_new > bleu4\n",
        "  bleu4 = max(bleu4_new, bleu4)\n",
        "  if not flag1:\n",
        "    epochs_since_improvement += 1\n",
        "  else:\n",
        "    epochs_since_improvement = 0\n",
        "'''"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/checkpoints/resnet152-b121ed2d.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16af402f76374ede8a3a3c5ef71eb5c8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=241530880.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch [0/20], Step: 0, Loss: 9.9471, Perplexity: 20891.0642\n",
            "Epoch [0/20], Step: 1, Loss: 8.1101, Perplexity: 3327.8911\n",
            "Epoch [0/20], Step: 2, Loss: 6.1173, Perplexity: 453.6281\n",
            "Epoch [1/20], Step: 0, Loss: 4.3611, Perplexity: 78.3423\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-f9793ad998f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m       grad_clip=grad_clip)\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0;31m# Validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-b8d51219b9c8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, encoder, decoder, criterion, decoder_optimizer, epoch, device, grad_clip)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0;31m# Output encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0;31m# Forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-37d5e8ad5544>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    348\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    349\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 350\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7GBTAINM2gH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FUNCTION THAT GIVEN AN IMAGE (KEY) PRINTS IT AND THE PRODUCED CAPTION\n",
        "def generate_caption(image_key,encoder,decoder,vocab):\n",
        "  # print the image\n",
        "  print_image_from_key(image_key)\n",
        "  # image preprocessing\n",
        "  input_image = transform_image(image_key)\n",
        "  # CNN\n",
        "  features = encoder(input_image)\n",
        "  # RNN\n",
        "  decoder_output = decoder.sample(features) \n",
        "  # get the caption\n",
        "  caption = tensor_to_word(decoder_output, vocab)\n",
        "  print(caption)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2OihxLZiKMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We need to balance the lenght of the output of the NN for test purpouses\n",
        "def paddingFill(idx_list, max_size):\n",
        "\n",
        "  i = len(idx_list)\n",
        "  for k in range(i,max_size):\n",
        "    idx_list.append(1)\n",
        "\n",
        "  return idx_list\n",
        "\n",
        "def test(test_loader, encoder, decoder, criterion, device, vocab_size):\n",
        "\n",
        "  decoder.eval()\n",
        "  with torch.no_grad():\n",
        "\n",
        "    total_loss = 0\n",
        "    for i, (image, caption, captionlen) in enumerate(test_loader):\n",
        "\n",
        "      image = image.to(device)\n",
        "      caption = caption.to(device)\n",
        "\n",
        "      for k_img, k_capt in zip(image,caption):\n",
        "\n",
        "        img = k_img.unsqueeze(0)\n",
        "        img = encoder(img)\n",
        "        features = img.unsqueeze(1)\n",
        "\n",
        "        decoder_out = decoder.sample(features)\n",
        "        decoder_out = paddingFill(decoder_out, len(k_capt))\n",
        "\n",
        "        decoder_out = torch.tensor(decoder_out, dtype=torch.int64)\n",
        "        #decoder_out = decoder_out.to(device)\n",
        "        embed_out = decoder.embedding_captions_layer(decoder_out)\n",
        "\n",
        "        print(\"Embed_out shape: \", embed_out.shape)\n",
        "        print(\"Caption shape: \", k_capt.shape)\n",
        "\n",
        "        loss = criterion(embed_out, k_capt) #TODO: this might not work and we might need to check the representation of decoder_out and caption\n",
        "        total_loss += loss\n",
        "    \n",
        "    print(\"Total loss: {} %\".format(total_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pYEWP2UNBQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inpt = torch.randn(3, 5, requires_grad=True)\n",
        "target = torch.empty(3, dtype=torch.long).random_(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3E9XW8fPrMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inpt.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPqLwRvxPr3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZL1pdFRPtKR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test(val_loader, \n",
        "     encoder=encoder, \n",
        "     decoder=decoder,\n",
        "     criterion=criterion,\n",
        "     device=device,\n",
        "     vocab_size=vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyXpV1pDRZD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}