{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SML_Project_Claudia.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/earendil94/SMLExam/blob/master/SML_Project_Claudia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLD803bQHlrZ",
        "colab_type": "text"
      },
      "source": [
        "**STATISTICAL MACHINE LEARNING**\n",
        "\n",
        "ARRIGHI Leonardo, BRAND Francesco, DORIGO Claudia\n",
        "\n",
        "\n",
        "Dataset folder is saved in \"/content/drive/My Drive/SML/SML_Project\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADn7VmfFHe_a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "efcf078d-b1a1-4497-bbcd-f2bda92bd973"
      },
      "source": [
        "# link colab and drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "# then follow passages"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqQgtn1AuSAi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c47a4254-ba80-4ace-b373-bf3e9d92f3d4"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import os\n",
        "import torchvision\n",
        "from torchvision import transforms, models\n",
        "from IPython import display\n",
        "import shelve\n",
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "torch.manual_seed(160898)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Device: {}'.format(device))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNq2wrM91FDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# APPLY TRANSFORMATIONS TO PIL IMAGE \n",
        "def transform_image(image):\n",
        "  transform = transforms.Compose([transforms.Resize(256),\n",
        "                                  transforms.CenterCrop(224),\n",
        "                                  transforms.ToTensor(),\n",
        "                                  transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "                                  ])\n",
        "  return transform(image)\n",
        "\n",
        "img_prova = Image.open('/content/drive/My Drive/SML/SML_Project/Chunk1/img1_200/1624481.jpg')\n",
        "i = transform_image(img_prova)\n",
        "\n",
        "# PRINT IMAGE HAVING AN IMAGE KEY\n",
        "def print_image_from_key(image_key):\n",
        "  path = '/content/drive/My Drive/SML/SML_Project/All_images/'\n",
        "  im = Image.open(os.path.join(path,str(image_key)+'.jpg'))\n",
        "  %matplotlib inline\n",
        "  imshow(np.asarray(im))\n",
        "\n",
        "#print_image_from_key(1624481)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttU_dLUsvIEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FUNCTION TO BUILD THE DICTIONARY (JUST FIRST 15K IMAGES)\n",
        "# don't run, it takes 2 hours\n",
        "\n",
        "def build_img_shelve(img_folder,shelve_path):\n",
        "  path = os.path.join(img_folder,'*.jpg')\n",
        "  with shelve.open(shelve_path) as d:\n",
        "    for i in glob.glob(path)[:15000]: #just first 15000 otherwise too big\n",
        "      im=Image.open(i)\n",
        "      d[i[len(img_folder):].split('.')[0]]=transform_image(im)\n",
        "\n",
        "    \n",
        "imgs = '/content/drive/My Drive/SML/SML_Project/All_images/'\n",
        "#build_img_shelve(imgs,'/content/drive/My Drive/SML/SML_Project/img_shelve_15k')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lfa2fpOVJjuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DICTIONARY FROM SHELVE\n",
        "def dict_from_shelve(shelve_file):\n",
        "  dictionary = {}\n",
        "  d=shelve.open(shelve_file)\n",
        "  for k in d.keys():\n",
        "    dictionary[int(k)]=d[k]\n",
        "  return dictionary\n",
        "\n",
        "chunk1_dict = dict_from_shelve('/content/drive/My Drive/SML/SML_Project/Chunk1/img_shelve')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQiPGK8Kxtl8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e8b801b0-b70c-40d1-821b-4bc4b9ae413f"
      },
      "source": [
        "#https://towardsdatascience.com/automatic-image-captioning-with-cnn-rnn-aae3cd442d83\n",
        "# ENCODER CNN\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        resnet = models.resnet152(pretrained=True)\n",
        "        for param in resnet.parameters():\n",
        "            param.requires_grad_(False)\n",
        "        \n",
        "        modules = list(resnet.children())[:-1] # remove last layer\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.resnet(images)\n",
        "        features = features.view(features.size(0), -1)\n",
        "        features = self.embed(features)\n",
        "        return features\n",
        "\n",
        "'''\n",
        "image = a[10002456]\n",
        "image = image.unsqueeze(0)\n",
        "cnn = EncoderCNN()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  image = image.to('cuda')\n",
        "  cnn.to('cuda')\n",
        "\n",
        "output = cnn(image)\n",
        "print(output,output.shape)\n",
        "'''"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nimage = a[10002456]\\nimage = image.unsqueeze(0)\\ncnn = EncoderCNN()\\n\\nif torch.cuda.is_available():\\n  image = image.to('cuda')\\n  cnn.to('cuda')\\n\\noutput = cnn(image)\\nprint(output,output.shape)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu65feqmxEBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FUNCTION TO CHECK THE RESULT OF CNN \n",
        "#(without removing last layer print 10 more probable classes) \n",
        "import json\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "def CNN_classification(image,model):\n",
        "  path = '/content/drive/My Drive/SML/SML_Project/All_images/'\n",
        "  im = Image.open(os.path.join(path,str(image)+'.jpg'))\n",
        "  model.eval()\n",
        "\n",
        "  out = model(transform_image(im).unsqueeze(0))\n",
        "  #class_probs = torch.nn.functional.softmax(out, dim=0) #not needed\n",
        "\n",
        "  class_idx = json.load(open(\"/content/drive/My Drive/SML/SML_Project/classes.json\"))\n",
        "  idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]\n",
        "\n",
        "  # sort wrt probability and take 10 more probable indexes\n",
        "  for idx in out[0].sort()[1][-10:]:\n",
        "    print(idx2label[idx])\n",
        "\n",
        "  #print the image\n",
        "  %matplotlib inline\n",
        "  imshow(np.asarray(im))\n",
        "\n",
        "#pretrained_ResNet = torch.hub.load('pytorch/vision:v0.6.0', 'resnet152', pretrained=True)\n",
        "#CNN_classification('1624481',pretrained_ResNet)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fL25f6WEjfU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DATASET,SAMPLER CLASSES (FOR DATALOADER) AND SPLIT METHODS\n",
        "import torch\n",
        "from torch.utils.data import Dataset, Sampler, DataLoader, SubsetRandomSampler\n",
        "import pandas as pd\n",
        "\n",
        "class imageCaptionDataset(Dataset):\n",
        "  '''\n",
        "    What we want to achieve is a mapping of type:\n",
        "    { img_name: [img_tensor, caption_1, caption_2, caption_3, caption_4, caption_5]}\n",
        "  '''\n",
        "\n",
        "  #TODO: should have a function that puts the two items in the same dictionary\n",
        "  def __init__(self, preProcessedImages, preProcessedCaptions):\n",
        "    '''\n",
        "      This function takes in input two dictionaries and merge them in a big_dictionary\n",
        "      according to common key values\n",
        "    '''\n",
        "    self.big_Dict = {}\n",
        "    for k in preProcessedCaptions.keys():\n",
        "      a = preProcessedImages.get(k) # return none if it doesn't exist\n",
        "      if a is not None:\n",
        "        self.big_Dict[k] = [a,preProcessedCaptions[k]]\n",
        "\n",
        "  def __getitem__(self, key):\n",
        "    '''\n",
        "      This function returns the preprocessed image and the first preprocessed comment \n",
        "      associated with the given key\n",
        "      TO DO:\n",
        "      - introduce a function to select the best comment\n",
        "      - define the behaviour when a key is not present\n",
        "    '''\n",
        "    #which_comment = 0 # get first comment - we can add here a function to select the best\n",
        "    comments = self.big_Dict[key][1]\n",
        "    returned_comment = comments.get('Caption_1')\n",
        "    return self.big_Dict[key][0], returned_comment, returned_comment.size()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.big_Dict)\n",
        "\n",
        "  def get_keys(self):\n",
        "    '''\n",
        "      I need this function in the sampler. It returns a list with all the \n",
        "      keys present in the big dictionary\n",
        "    '''\n",
        "    return self.big_Dict.keys()\n",
        "\n",
        "\n",
        "class imageCaptionSampler(Sampler):\n",
        "  '''\n",
        "    TO DO: \n",
        "    - _iter_ which returns an iterable over the dataset\n",
        "    - _len_ which returns the length of the dataset (needed to compute number of batches in dataloader)\n",
        "  '''\n",
        "  def __init__(self, data_source):\n",
        "    self.data_source = data_source\n",
        "\n",
        "  def __iter__(self):\n",
        "    '''\n",
        "      we don't care the order in which iterate the dataset so this function defines\n",
        "      an iterator over the key list\n",
        "    '''\n",
        "    return iter(self.data_source.get_keys())\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data_source.get_keys())  \n",
        "\n",
        "\n",
        "\n",
        "def split(imgDataset, val_size):\n",
        "  '''\n",
        "    @imgDataset: an image caption dataset object\n",
        "    @val_size: percentage of the dataset that should compose the validation set\n",
        "\n",
        "    This function allows us to split our dataset into \n",
        "    a validation set and a training set. This is used internally in \n",
        "    Loader, should check that one.\n",
        "  '''\n",
        "\n",
        "  # We want to split our dataset given itself and the % of sample for validation\n",
        "  num = len(imgDataset)\n",
        "  index = list(imgDataset.get_keys())\n",
        "  np.random.shuffle(index) # pick at random\n",
        "  flag_split = int(val_size * num)\n",
        "\n",
        "  train_index = index[flag_split:]\n",
        "  validation_index = index[:flag_split]\n",
        "\n",
        "  # https://pytorch.org/docs/stable/data.html -> Samples elements randomly from a given list of indices, without replacement\n",
        "  train_sampler = SubsetRandomSampler(train_index)\n",
        "  validation_sampler = SubsetRandomSampler(validation_index)\n",
        "\n",
        "  return train_sampler, validation_sampler\n",
        "\n",
        "def loaders(dataset, val_size, batch_size, num_workers):\n",
        "  ''' \n",
        "    @dataset: an image caption dataset object\n",
        "    @val_size: the percentage (must be [0,1]) of the validation set data\n",
        "    @batch_size: the number of data in each batch\n",
        "    @num_workers: number of subprocesses to use in the data loader\n",
        "  '''\n",
        "\n",
        "  train_sampler, validation_sampler = split(dataset, val_size)\n",
        "  train_loader = DataLoader(dataset,\n",
        "                            batch_size = batch_size,\n",
        "                            sampler = train_sampler,\n",
        "                            num_workers = num_workers)\n",
        "  val_loader = DataLoader(dataset,\n",
        "                          batch_size = batch_size,\n",
        "                          sampler = validation_sampler,\n",
        "                          num_workers = num_workers)\n",
        "  return train_loader, val_loader"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXwd8gvh4GyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CAPTION PREPROCESSING FUNCTIONS\n",
        "from torchtext.data import Field\n",
        "from torchtext.data import TabularDataset\n",
        "from torchtext.data import Iterator\n",
        "\n",
        "def prepare_data(path, input_file, output_file):\n",
        "  input_path = os.path.join(path, input_file)\n",
        "  output_path = os.path.join(path, output_file)\n",
        "  df = pd.read_csv(input_path, sep = \"|\")\n",
        "  captions_array = df[' comment']\n",
        "  captions_array.fillna(\"\", inplace=True)\n",
        "  \n",
        "  image_names = df[\"image_name\"].values\n",
        "  image_number = []\n",
        "  for i in range(0, len(image_names)):\n",
        "    image_number.append(image_names[i].split('.')[0])\n",
        "\n",
        "  df.drop(labels=['image_name', ' comment_number'], axis=1, inplace=True)\n",
        "  df.index = image_number\n",
        "  df.to_csv(output_path, index_label=\"image_number\")\n",
        "  return output_path\n",
        "\n",
        "def build_vocab(path_to_caption_file, caption_file):\n",
        "  output_path = prepare_data(path_to_caption_file, caption_file, \"clean.csv\")\n",
        "  tokenize = lambda x : x.split()\n",
        "  TEXT = Field(sequential = True, tokenize = tokenize, lower=True, init_token='<start>', eos_token='<end>')\n",
        "  LABEL = Field(sequential=False, use_vocab=False)\n",
        "  td_datafields = [(\"image_number\", LABEL ),\n",
        "                  (\"comment\", TEXT)]\n",
        "\n",
        "  trn = TabularDataset(\n",
        "              path=output_path, # the root directory where the data lies\n",
        "              format='csv',\n",
        "              skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
        "              fields=td_datafields,\n",
        "              )\n",
        "  \n",
        "  TEXT.build_vocab(trn)\n",
        "  return TEXT.vocab\n",
        "\n",
        "def word_caption_to_index(path_to_caption_file, caption_file):\n",
        "  '''\n",
        "    This function takes as input the file containing the captions\n",
        "    and returns a matrix of the captions indexed with respect to the inner\n",
        "    vocabulary, as well as an array that can be used to map the indexed caption\n",
        "    to the image it belongs to.\n",
        "    @path_to_caption_file: the path to file containing the captions\n",
        "    @caption_file: the name of the file containing the captions\n",
        "  '''\n",
        "\n",
        "  output_path = prepare_data(path_to_caption_file, caption_file, \"clean.csv\")\n",
        "  tokenize = lambda x : x.split()\n",
        "  TEXT = Field(sequential = True, tokenize = tokenize, lower=True, init_token='<start>', eos_token='<end>')\n",
        "  LABEL = Field(sequential=False, use_vocab=False)\n",
        "\n",
        "  td_datafields = [(\"image_number\", LABEL ),\n",
        "                  (\"comment\", TEXT)]\n",
        "\n",
        "  trn = TabularDataset(\n",
        "                path=output_path, # the root directory where the data lies\n",
        "                format='csv',\n",
        "                skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
        "                fields=td_datafields\n",
        "                )\n",
        "  \n",
        "  TEXT.build_vocab(trn)\n",
        "  train_iter = Iterator(trn, batch_size=len(trn), device = -1)\n",
        "\n",
        "  for i in train_iter: #TODO:I really havent got it how this still works honestly\n",
        "    cmt = i.comment\n",
        "    img = i.image_number\n",
        "\n",
        "  return cmt.T, img\n",
        "\n",
        "def vocab_as_dict(path_to_caption_file, caption_file):\n",
        "  vocab = build_vocab(path_to_caption_file, caption_file)\n",
        "  return vocab.stoi\n",
        "\n",
        "def tensor_to_word(indexed_word, vocab):\n",
        "  for i in indexed_word:\n",
        "    k = i.item()\n",
        "    if k == 1:\n",
        "      break\n",
        "    else:\n",
        "      print(vocab.itos[k], end = \" \")\n",
        "    \n",
        "  \n",
        "\n",
        "def get_caption_from_image(caption_indexes, caption_refs, image_number):\n",
        "  #If image name actually has the .jpg tail\n",
        "  caption_refs = (caption_refs == int(image_number))\n",
        "  caption_refs = caption_refs.nonzero().T.numpy()[0].tolist()\n",
        "  return caption_indexes[caption_refs]\n",
        "\n",
        "def buildCaptionDict(path_to_caption_file, caption_file):\n",
        "\n",
        "  caption_index, refs = word_caption_to_index(path_to_caption_file, caption_file)\n",
        "  refs_list = refs.numpy().tolist()\n",
        "  refs_set = set(refs_list)\n",
        "  unique_refs_list = list(refs_set)\n",
        "\n",
        "  df = []\n",
        "  df = pd.DataFrame(columns=[\"Image_number\", \"Caption_1\", \"Caption_2\", \"Caption_3\", \"Caption_4\", \"Caption_5\"])\n",
        "\n",
        "  for i in range(0, len(unique_refs_list)):\n",
        "    capt_1 = get_caption_from_image(caption_index, refs, unique_refs_list[i])[0]\n",
        "    capt_2 = get_caption_from_image(caption_index, refs, unique_refs_list[i])[1]\n",
        "    capt_3 = get_caption_from_image(caption_index, refs, unique_refs_list[i])[2]\n",
        "    capt_4 = get_caption_from_image(caption_index, refs, unique_refs_list[i])[3]\n",
        "    capt_5 = get_caption_from_image(caption_index, refs, unique_refs_list[i])[4]\n",
        "    df.loc[i] = [unique_refs_list[i], capt_1, capt_2, capt_3, capt_4, capt_5]\n",
        "\n",
        "  df.set_index('Image_number', inplace=True)\n",
        "  capt_dict = df.to_dict('index')\n",
        "\n",
        "  return capt_dict"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syUMQ1Hs293g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4b17daf0-43c2-486b-9eb5-ad8a38cc4060"
      },
      "source": [
        "# TEST ON OUR DATA\n",
        "# image dictionary\n",
        "chunk1_dict = dict_from_shelve('/content/drive/My Drive/SML/SML_Project/Chunk1/img_shelve')\n",
        "\n",
        "# caption dictionary\n",
        "path_to_caption_file = \"/content/drive/My Drive/SML/SML_Project/Chunk1\"\n",
        "caption_file = \"results.csv\"\n",
        "caption_dict = buildCaptionDict(path_to_caption_file, caption_file)\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loHiiy6k7tFO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "outputId": "48b652a2-f0f5-408c-d5ff-541520193d5f"
      },
      "source": [
        "dataset = imageCaptionDataset(chunk1_dict,caption_dict)\n",
        "#chunk1_dict['1624481']\n",
        "img_tensor, caption_tensor, size = dataset[1624481]\n",
        "print(img_tensor)\n",
        "print(caption_tensor)\n",
        "print(size)\n",
        "dataset.get_keys()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-0.7822, -0.6794, -0.6109,  ...,  0.2453,  0.4508,  0.8276],\n",
            "         [-0.8678, -0.8507, -0.8678,  ..., -0.6794, -0.5596, -0.4226],\n",
            "         [-0.8849, -0.8507, -0.8507,  ..., -0.7479, -0.7308, -0.7650],\n",
            "         ...,\n",
            "         [ 0.8447,  0.8447,  0.8276,  ...,  0.3309,  0.3481,  0.3481],\n",
            "         [ 0.9303,  0.9474,  0.9303,  ...,  0.3652,  0.3652,  0.3652],\n",
            "         [ 0.9132,  0.9303,  0.9303,  ...,  0.4166,  0.3823,  0.3652]],\n",
            "\n",
            "        [[-0.6352, -0.4951, -0.3901,  ...,  0.4678,  0.6779,  1.0455],\n",
            "         [-0.7227, -0.6702, -0.6527,  ..., -0.5476, -0.4251, -0.2675],\n",
            "         [-0.7402, -0.6702, -0.6527,  ..., -0.7402, -0.7227, -0.7227],\n",
            "         ...,\n",
            "         [ 1.0980,  1.0980,  1.0805,  ...,  0.5728,  0.5903,  0.5903],\n",
            "         [ 1.1856,  1.2031,  1.1856,  ...,  0.6078,  0.6078,  0.6078],\n",
            "         [ 1.1681,  1.1856,  1.1856,  ...,  0.6604,  0.6254,  0.6078]],\n",
            "\n",
            "        [[-0.6367, -0.5321, -0.4798,  ...,  0.2522,  0.4614,  0.8622],\n",
            "         [-0.7064, -0.7064, -0.6890,  ..., -0.6890, -0.5670, -0.4450],\n",
            "         [-0.7064, -0.6890, -0.6193,  ..., -0.7936, -0.7936, -0.8110],\n",
            "         ...,\n",
            "         [ 1.2457,  1.2457,  1.2282,  ...,  0.7228,  0.7402,  0.7402],\n",
            "         [ 1.3328,  1.3502,  1.3328,  ...,  0.7576,  0.7576,  0.7576],\n",
            "         [ 1.3154,  1.3328,  1.3328,  ...,  0.8099,  0.7751,  0.7576]]])\n",
            "tensor([    2, 10034,  2415,   185,   477,    13,  1104,   909,    40,    38,\n",
            "           14,    74,     6,  2797,   378,     9,    44,  4896,     5,     3,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1])\n",
            "torch.Size([84])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys([7340189, 134206, 5377361, 5771732, 4985704, 4199555, 1317156, 793558, 2760167, 10101477, 3680138, 3025093, 6827028, 6696219, 5648321, 667626, 8664920, 8664922, 3160699, 5914327, 8404753, 5521996, 148284, 9324151, 675153, 3035057, 5918675, 5918840, 807129, 5919020, 5526034, 1989609, 5791070, 5791244, 5791568, 6054169, 5400154, 9726060, 5402085, 3043766, 8680922, 4749855, 2784746, 9600569, 4489731, 8684718, 8029536, 2656351, 36979, 301246, 7510394, 6331511, 10002456, 960092, 6335241, 5287405, 438106, 178045, 6338704, 6338733, 6339096, 10010052, 10404007, 7520721, 7520731, 6734417, 574181, 4376178, 8832804, 1624481, 2148982, 4378823, 5558592, 2806447, 6214447, 7656601, 8443156, 7527111, 4906946, 2285664, 1369162, 4515460, 5958182, 4386588, 5566972, 10287332, 1243756, 3996401, 3734864, 5570219, 5570254, 65567, 8454235, 7013217, 984950, 8063007, 8849890, 2689611, 7015055, 854749, 10160966, 9637989, 5444724, 6100315, 726414, 4135695, 3219606, 205842, 7808046, 1254659, 4926723, 8859482, 8990627, 5060753, 8206921, 8601145, 81641, 3753939, 1920465, 6901333, 6901412, 6901479, 4280272, 3494059, 5722658, 4413714, 4414061, 2317271, 6905083, 3367399, 5333578, 7300624, 7300628, 5858851, 353913, 5729814, 9268793, 881336, 8876934, 2192573, 10188041, 3765589, 490870, 5733760, 3637013, 4162702, 5867606, 6261030, 2069887, 1283466, 4429660, 6920532, 5871970, 10459869, 5217116, 6659698, 8757787, 5088155, 6398924, 6398952, 6398988, 764507, 371897, 371902, 371903, 7188003, 6664030, 2209317, 2209751, 4307968, 2868798, 3001353, 3787801, 9556225, 9556226, 5230968, 5624522, 4183120, 4576671, 9950858, 9950913, 9426826, 10082347, 10082348, 8378599, 9035232, 5104045, 3662865, 5629300, 256063, 6678735, 6155176, 10350842, 7598674, 7598946, 390369, 3012229, 10090841, 3537322, 1440465])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_by1X2Gjxnk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO: Right now this is basically the same exact class described in the towards data science article\n",
        "#Should/Do we have to make any changes to this?\n",
        "class DecoderRNN(nn.Module): \n",
        "  def __init__(self, embed_size, hidden_size, vocab_size, num_layers = 1): #Here we define the layers\n",
        "    super().__init__()\n",
        "    self.embedding_captions_layer = nn.Embedding(vocab_size, embed_size)\n",
        "    self.LSTM = nn.LSTM(input_size = embed_size, hidden_size = hidden_size, \n",
        "                        num_layers = num_layers, batch_first = True)\n",
        "    self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "  def forward(self, features, captions): #Notice that features will only be used when we will have the encoder images in input\n",
        "    captions = captions[:, :-1]\n",
        "    embed = self.embedding_captions_layer(captions)\n",
        "    #A couple of comments here: torch.cat concatenates just like in unix\n",
        "    #torch.unsqueeze instead transforms the tensor into a column vector (column since we specify 1 here)\n",
        "    embed = torch.cat((features.unsqueeze(1), embed), dim = 1) \n",
        "    lstm_outputs, _ = self.LSTM(embed)\n",
        "    out = self.linear(lstm_outputs)\n",
        "    return out\n",
        "\n",
        "  def sample(self, inputs, states=None, max_len=20):\n",
        "    output_sentence = []\n",
        "\n",
        "    for i in range(max_len):\n",
        "      lstm_outputs, states = self.lstm(inputs, states)\n",
        "      lstm_outputs = lstm_outputs.squeeze(1)\n",
        "      out = self.linear(lstm_outputs)\n",
        "      last_pick = out.max(1)[1]\n",
        "      output_sentence.append(last_pick.item())\n",
        "      inputs = self.embedding_layer(last_pick).unsqueeze(1)\n",
        "      \n",
        "    return output_sentence"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CF7Q1WQvDhX-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#SOME TEST REGARDING SAMPLER AND DATALOADER\n",
        "\n",
        "# initialize a sampler\n",
        "#sampler = imageCaptionSampler(dataset)\n",
        "\n",
        "# initialize a dataloader using these 2 new classes\n",
        "#dataloader = DataLoader(dataset, batch_size=10, sampler=sampler, num_workers=1)\n",
        "\n",
        "# check if iteration works\n",
        "#dataiter = iter(dataloader)\n",
        "#print(dataiter.next())\n",
        "#print(dataiter.next()) \n",
        "#len(dataloader)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjvEY6Ip1eIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#PARAMETERS DEFINITION\n",
        "\n",
        "\n",
        "#from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "# RNN - Model parameters\n",
        "hidden_size = 1000\n",
        "embed_size = 400\n",
        "vocab_size = len(vocab_as_dict('/content/drive/My Drive/SML/SML_Project/Chunk1/', 'results.csv'))\n",
        "dropout = 0.5 # suggested\n",
        "\n",
        "# Parameters\n",
        "epoch = 20\n",
        "start_epoch = 0\n",
        "epochs_since_improvement = 0 # keeps track of number of epochs since there's been an improvement in validation BLEU\n",
        "batch_size = 64\n",
        "decoder_lr = 5e-4  # learning rate\n",
        "checkpoint = None\n",
        "bleu4 = 0. # bleu score -> https://www.aclweb.org/anthology/P02-1040.pdf\n",
        "workers = 1 # for dataloaders: how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process\n",
        "grad_clip = 5. # must be changed after all "
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehazjG4c1mN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# maybe this should be saved in a utils.py file\n",
        "\n",
        "def adjust_learning_rate(optimizer, shrink_factor):\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = param_group['lr'] * shrink_factor\n",
        "\n",
        "def clip_gradient(optimizer, grad_clip):\n",
        "  for group in optimizer.param_groups:\n",
        "    for param in group['params']:\n",
        "      if param.grad is not None:\n",
        "        param.grad.data.clamp_(-grad_clip, grad_clip)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifWMIAWg2OgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_loader, encoder, decoder, criterion, decoder_optimizer, epoch, device, grad_clip):\n",
        "\n",
        "  decoder.train() # train mode\n",
        "\n",
        "  # Epochs\n",
        "  for i in range(epoch):\n",
        "    \n",
        "    # Batch\n",
        "    for i_step, (image, caption, captionlen) in enumerate(train_loader):\n",
        "      \n",
        "      # Move to GPU, if available\n",
        "      image = image.to(device)\n",
        "      caption = caption.to(device)\n",
        "      #captionlen = captionlen.to(device)\n",
        "\n",
        "      # Output encoder\n",
        "      image = encoder(image)\n",
        "\n",
        "      # Forward \n",
        "      decoder_output = decoder(image, caption)\n",
        "      \n",
        "      \n",
        "      # Calculate loss\n",
        "      loss = criterion(decoder_output.view(-1, vocab_size), caption.view(-1))\n",
        "      #loss = criterion(decoder_output, caption)\n",
        "      #print('Loss = ', loss)\n",
        "      print_every = 1\n",
        "      stats = 'Epoch [%d/%d], Step: %d, Loss: %.4f, Perplexity: %5.4f' % (i, epoch, i_step, loss.item(), np.exp(loss.item()))\n",
        "      if i_step % print_every == 0:\n",
        "          print('\\r' + stats)\n",
        "    \n",
        "      # Back propagation\n",
        "      decoder_optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      \n",
        "\n",
        "      # Clip gradients\n",
        "      # https://machinelearningmastery.com/how-to-avoid-exploding-gradients-in-neural-networks-with-gradient-clipping/\n",
        "      if grad_clip is not None:\n",
        "        clip_gradient(decoder_optimizer, grad_clip)\n",
        "\n",
        "      # Update weights\n",
        "      decoder_optimizer.step()\n"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aITyb41r1oMG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "f625373b-a567-439b-be4e-70fb423264f1"
      },
      "source": [
        "# Encoder\n",
        "encoder = EncoderCNN(embed_size)\n",
        "\n",
        "# Decoder\n",
        "decoder = DecoderRNN(embed_size, hidden_size, vocab_size) #dropout = dropout)\n",
        "\n",
        "decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, \n",
        "                                                   decoder.parameters()),\n",
        "                                     lr=decoder_lr)\n",
        "\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# DataLoader\n",
        "train_loader, val_loader = loaders(dataset, 0.2, batch_size, 1)\n",
        "\n",
        "'''\n",
        "# Epochs\n",
        "for epoch in range(start_epoch, epochs):\n",
        "  # Decay learning rate if there is no improvement for 8 consecutive epochs, and terminate training after 20\n",
        "  if epochs_since_improvement == 20:\n",
        "    break\n",
        "  if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n",
        "    adjust_learning_rate(decoder_optimizer, 0.8)\n",
        "'''\n",
        "\n",
        "# Training\n",
        "train(train_loader=train_loader,\n",
        "      encoder=encoder,\n",
        "      decoder=decoder,\n",
        "      criterion=criterion,\n",
        "      decoder_optimizer=decoder_optimizer,\n",
        "      epoch=epoch,\n",
        "      device=device,\n",
        "      grad_clip=grad_clip)\n",
        "\n",
        "  # Validation\n",
        "  #bleu4_new\n",
        "\n",
        "'''\n",
        "  # BLEU4\n",
        "  flag1 = bleu4_new > bleu4\n",
        "  bleu4 = max(bleu4_new, bleu4)\n",
        "  if not flag1:\n",
        "    epochs_since_improvement += 1\n",
        "  else:\n",
        "    epochs_since_improvement = 0\n",
        "'''"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [0/5], Step: 0, Loss: 10.0190, Perplexity: 22448.9281\n",
            "Epoch [0/5], Step: 1, Loss: 9.6581, Perplexity: 15648.2212\n",
            "Epoch [0/5], Step: 2, Loss: 9.2917, Perplexity: 10847.1892\n",
            "Epoch [1/5], Step: 0, Loss: 8.9234, Perplexity: 7505.5714\n",
            "Epoch [1/5], Step: 1, Loss: 8.5407, Perplexity: 5118.7699\n",
            "Epoch [1/5], Step: 2, Loss: 8.1561, Perplexity: 3484.4250\n",
            "Epoch [2/5], Step: 0, Loss: 7.7149, Perplexity: 2241.4229\n",
            "Epoch [2/5], Step: 1, Loss: 7.2736, Perplexity: 1441.7453\n",
            "Epoch [2/5], Step: 2, Loss: 6.7613, Perplexity: 863.7999\n",
            "Epoch [3/5], Step: 0, Loss: 6.2640, Perplexity: 525.3019\n",
            "Epoch [3/5], Step: 1, Loss: 5.7991, Perplexity: 330.0173\n",
            "Epoch [3/5], Step: 2, Loss: 5.2469, Perplexity: 189.9821\n",
            "Epoch [4/5], Step: 0, Loss: 4.7262, Perplexity: 112.8670\n",
            "Epoch [4/5], Step: 1, Loss: 4.2487, Perplexity: 70.0114\n",
            "Epoch [4/5], Step: 2, Loss: 3.7953, Perplexity: 44.4928\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n  # BLEU4\\n  flag1 = bleu4_new > bleu4\\n  bleu4 = max(bleu4_new, bleu4)\\n  if not flag1:\\n    epochs_since_improvement += 1\\n  else:\\n    epochs_since_improvement = 0\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7GBTAINM2gH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FUNCTION THAT GIVEN AN IMAGE (KEY) PRINTS IT AND THE PRODUCED CAPTION\n",
        "def generate_caption(image_key,encoder,decoder,vocab):\n",
        "  # print the image\n",
        "  print_image_from_key(image_key)\n",
        "  # image preprocessing\n",
        "  input_image = transform_image(image_key)\n",
        "  # CNN\n",
        "  features = encoder(input_image)\n",
        "  # RNN\n",
        "  decoder_output = decoder.sample(features) \n",
        "  # get the caption\n",
        "  caption = tensor_to_word(decoder_output, vocab)\n",
        "  print(caption)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2OihxLZiKMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    }
  ]
}