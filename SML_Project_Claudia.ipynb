{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SML_Project_Claudia.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/earendil94/SMLExam/blob/master/SML_Project_Claudia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLD803bQHlrZ",
        "colab_type": "text"
      },
      "source": [
        "**STATISTICAL MACHINE LEARNING**\n",
        "\n",
        "ARRIGHI Leonardo, BRAND Francesco, DORIGO Claudia\n",
        "\n",
        "\n",
        "Dataset folder is saved in \"/content/drive/My Drive/SML/SML_Project\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADn7VmfFHe_a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "a17efb10-78c1-4182-cbe2-bfb99fee3e22"
      },
      "source": [
        "# link colab and drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "# then follow passages"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqQgtn1AuSAi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "768d9150-a4e6-4fc8-fd82-f32a9f49f886"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import os\n",
        "import torchvision\n",
        "from torchvision import transforms, models\n",
        "from IPython import display\n",
        "import shelve\n",
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "torch.manual_seed(160898)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Device: {}'.format(device))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNq2wrM91FDA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "bad20279-ea67-4807-fa40-5a8392b65353"
      },
      "source": [
        "# APPLY TRANSFORMATIONS TO PIL IMAGE \n",
        "def transform_image(image):\n",
        "  transform = transforms.Compose([transforms.Resize(256),\n",
        "                                  transforms.CenterCrop(224),\n",
        "                                  transforms.ToTensor(),\n",
        "                                  transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "                                  ])\n",
        "  return transform(image)\n",
        "\n",
        "img_prova = Image.open('/content/drive/My Drive/SML/SML_Project/Chunk1/img1_200/1624481.jpg')\n",
        "transform_image(img_prova)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.7822, -0.6794, -0.6109,  ...,  0.2453,  0.4508,  0.8276],\n",
              "         [-0.8678, -0.8507, -0.8678,  ..., -0.6794, -0.5596, -0.4226],\n",
              "         [-0.8849, -0.8507, -0.8507,  ..., -0.7479, -0.7308, -0.7650],\n",
              "         ...,\n",
              "         [ 0.8447,  0.8447,  0.8276,  ...,  0.3309,  0.3481,  0.3481],\n",
              "         [ 0.9303,  0.9474,  0.9303,  ...,  0.3652,  0.3652,  0.3652],\n",
              "         [ 0.9132,  0.9303,  0.9303,  ...,  0.4166,  0.3823,  0.3652]],\n",
              "\n",
              "        [[-0.6352, -0.4951, -0.3901,  ...,  0.4678,  0.6779,  1.0455],\n",
              "         [-0.7227, -0.6702, -0.6527,  ..., -0.5476, -0.4251, -0.2675],\n",
              "         [-0.7402, -0.6702, -0.6527,  ..., -0.7402, -0.7227, -0.7227],\n",
              "         ...,\n",
              "         [ 1.0980,  1.0980,  1.0805,  ...,  0.5728,  0.5903,  0.5903],\n",
              "         [ 1.1856,  1.2031,  1.1856,  ...,  0.6078,  0.6078,  0.6078],\n",
              "         [ 1.1681,  1.1856,  1.1856,  ...,  0.6604,  0.6254,  0.6078]],\n",
              "\n",
              "        [[-0.6367, -0.5321, -0.4798,  ...,  0.2522,  0.4614,  0.8622],\n",
              "         [-0.7064, -0.7064, -0.6890,  ..., -0.6890, -0.5670, -0.4450],\n",
              "         [-0.7064, -0.6890, -0.6193,  ..., -0.7936, -0.7936, -0.8110],\n",
              "         ...,\n",
              "         [ 1.2457,  1.2457,  1.2282,  ...,  0.7228,  0.7402,  0.7402],\n",
              "         [ 1.3328,  1.3502,  1.3328,  ...,  0.7576,  0.7576,  0.7576],\n",
              "         [ 1.3154,  1.3328,  1.3328,  ...,  0.8099,  0.7751,  0.7576]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttU_dLUsvIEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FUNCTION TO BUILD THE DICTIONARY (JUST FIRST 15K IMAGES)\n",
        "# don't run, it takes 2 hours\n",
        "\n",
        "def build_img_shelve(img_folder,shelve_path):\n",
        "  path = os.path.join(img_folder,'*.jpg')\n",
        "  with shelve.open(shelve_path) as d:\n",
        "    for i in glob.glob(path)[:15000]: #just first 15000 otherwise too big\n",
        "      im=Image.open(i)\n",
        "      d[i[len(img_folder):].split('.')[0]]=transform_image(im)\n",
        "\n",
        "    \n",
        "imgs = '/content/drive/My Drive/SML/SML_Project/All_images/'\n",
        "#build_img_shelve(imgs,'/content/drive/My Drive/SML/SML_Project/img_shelve_15k')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lfa2fpOVJjuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DICTIONARY FROM SHELVE\n",
        "def dict_from_shelve(shelve_file):\n",
        "  dictionary = {}\n",
        "  d=shelve.open(shelve_file)\n",
        "  for k in d.keys():\n",
        "    dictionary[int(k)]=d[k]\n",
        "  return dictionary\n",
        "\n",
        "chunk1_dict = dict_from_shelve('/content/drive/My Drive/SML/SML_Project/Chunk1/img_shelve')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQiPGK8Kxtl8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d01dcebd-ccba-4b02-a347-3baa87792451"
      },
      "source": [
        "#https://towardsdatascience.com/automatic-image-captioning-with-cnn-rnn-aae3cd442d83\n",
        "# ENCODER CNN\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        resnet = models.resnet152(pretrained=True)\n",
        "        for param in resnet.parameters():\n",
        "            param.requires_grad_(False)\n",
        "        \n",
        "        modules = list(resnet.children())[:-1] # remove last layer\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.resnet(images)\n",
        "        features = features.view(features.size(0), -1)\n",
        "        features = self.embed(features)\n",
        "        return features\n",
        "\n",
        "'''\n",
        "image = a[10002456]\n",
        "image = image.unsqueeze(0)\n",
        "cnn = EncoderCNN()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  image = image.to('cuda')\n",
        "  cnn.to('cuda')\n",
        "\n",
        "output = cnn(image)\n",
        "print(output,output.shape)\n",
        "'''"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nimage = a[10002456]\\nimage = image.unsqueeze(0)\\ncnn = EncoderCNN()\\n\\nif torch.cuda.is_available():\\n  image = image.to('cuda')\\n  cnn.to('cuda')\\n\\noutput = cnn(image)\\nprint(output,output.shape)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu65feqmxEBR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "e072333d-c906-45a8-80a9-8d88310d5bd4"
      },
      "source": [
        "# FUNCTION TO CHECK THE RESULT OF CNN \n",
        "#(without removing last layer print 10 more probable classes) \n",
        "import json\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "def CNN_classification(image,model):\n",
        "  path = '/content/drive/My Drive/SML/SML_Project/All_images/'\n",
        "  im = Image.open(os.path.join(path,image+'.jpg'))\n",
        "  model.eval()\n",
        "\n",
        "  out = model(transform_image(im).unsqueeze(0))\n",
        "  #class_probs = torch.nn.functional.softmax(out, dim=0) #not needed\n",
        "\n",
        "  class_idx = json.load(open(\"/content/drive/My Drive/SML/SML_Project/classes.json\"))\n",
        "  idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]\n",
        "\n",
        "  # sort wrt probability and take 10 more probable indexes\n",
        "  for idx in out[0].sort()[1][-10:]:\n",
        "    print(idx2label[idx])\n",
        "\n",
        "  #print the image\n",
        "  %matplotlib inline\n",
        "  imshow(np.asarray(im))\n",
        "\n",
        "pretrained_ResNet = torch.hub.load('pytorch/vision:v0.6.0', 'resnet152', pretrained=True)\n",
        "CNN_classification('1624481',pretrained_ResNet)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.6.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-b9544ca7e81e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mpretrained_ResNet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pytorch/vision:v0.6.0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'resnet152'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mCNN_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1624481'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpretrained_ResNet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-b9544ca7e81e>\u001b[0m in \u001b[0;36mCNN_classification\u001b[0;34m(image, model)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mCNN_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/SML/SML_Project/All_images/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2808\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2809\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2810\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 5] Input/output error: '/content/drive/My Drive/SML/SML_Project/All_images/1624481.jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fL25f6WEjfU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DATASET,SAMPLER CLASSES (FOR DATALOADER) AND SPLIT METHODS\n",
        "import torch\n",
        "from torch.utils.data import Dataset, Sampler, DataLoader, SubsetRandomSampler\n",
        "import pandas as pd\n",
        "\n",
        "class imageCaptionDataset(Dataset):\n",
        "  '''\n",
        "    What we want to achieve is a mapping of type:\n",
        "    { img_name: [img_tensor, caption_1, caption_2, caption_3, caption_4, caption_5]}\n",
        "  '''\n",
        "\n",
        "  #TODO: should have a function that puts the two items in the same dictionary\n",
        "  def __init__(self, preProcessedImages, preProcessedCaptions):\n",
        "    '''\n",
        "      This function takes in input two dictionaries and merge them in a big_dictionary\n",
        "      according to common key values\n",
        "    '''\n",
        "    self.big_Dict = {}\n",
        "    for k in preProcessedCaptions.keys():\n",
        "      a = preProcessedImages.get(k) # return none if it doesn't exist\n",
        "      if a is not None:\n",
        "        self.big_Dict[k] = [a,preProcessedCaptions[k]]\n",
        "\n",
        "  def __getitem__(self, key):\n",
        "    '''\n",
        "      This function returns the preprocessed image and the first preprocessed comment \n",
        "      associated with the given key\n",
        "      TO DO:\n",
        "      - introduce a function to select the best comment\n",
        "      - define the behaviour when a key is not present\n",
        "    '''\n",
        "    #which_comment = 0 # get first comment - we can add here a function to select the best\n",
        "    comments = self.big_Dict[key][1]\n",
        "    returned_comment = comments.get('Caption_1')\n",
        "    return self.big_Dict[key][0], returned_comment, returned_comment.size()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.big_Dict)\n",
        "\n",
        "  def get_keys(self):\n",
        "    '''\n",
        "      I need this function in the sampler. It returns a list with all the \n",
        "      keys present in the big dictionary\n",
        "    '''\n",
        "    return self.big_Dict.keys()\n",
        "\n",
        "\n",
        "class imageCaptionSampler(Sampler):\n",
        "  '''\n",
        "    TO DO: \n",
        "    - _iter_ which returns an iterable over the dataset\n",
        "    - _len_ which returns the length of the dataset (needed to compute number of batches in dataloader)\n",
        "  '''\n",
        "  def __init__(self, data_source):\n",
        "    self.data_source = data_source\n",
        "\n",
        "  def __iter__(self):\n",
        "    '''\n",
        "      we don't care the order in which iterate the dataset so this function defines\n",
        "      an iterator over the key list\n",
        "    '''\n",
        "    return iter(self.data_source.get_keys())\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data_source.get_keys())  \n",
        "\n",
        "\n",
        "\n",
        "def split(imgDataset, val_size):\n",
        "  '''\n",
        "    @imgDataset: an image caption dataset object\n",
        "    @val_size: percentage of the dataset that should compose the validation set\n",
        "\n",
        "    This function allows us to split our dataset into \n",
        "    a validation set and a training set. This is used internally in \n",
        "    Loader, should check that one.\n",
        "  '''\n",
        "\n",
        "  # We want to split our dataset given itself and the % of sample for validation\n",
        "  num = len(imgDataset)\n",
        "  index = list(imgDataset.get_keys())\n",
        "  np.random.shuffle(index) # pick at random\n",
        "  flag_split = int(val_size * num)\n",
        "\n",
        "  train_index = index[flag_split:]\n",
        "  validation_index = index[:flag_split]\n",
        "\n",
        "  # https://pytorch.org/docs/stable/data.html -> Samples elements randomly from a given list of indices, without replacement\n",
        "  train_sampler = SubsetRandomSampler(train_index)\n",
        "  validation_sampler = SubsetRandomSampler(validation_index)\n",
        "\n",
        "  return train_sampler, validation_sampler\n",
        "\n",
        "def loaders(dataset, val_size, batch_size, num_workers):\n",
        "  ''' \n",
        "    @dataset: an image caption dataset object\n",
        "    @val_size: the percentage (must be [0,1]) of the validation set data\n",
        "    @batch_size: the number of data in each batch\n",
        "    @num_workers: number of subprocesses to use in the data loader\n",
        "  '''\n",
        "\n",
        "  train_sampler, validation_sampler = split(dataset, val_size)\n",
        "  train_loader = DataLoader(dataset,\n",
        "                            batch_size = batch_size,\n",
        "                            sampler = train_sampler,\n",
        "                            num_workers = num_workers)\n",
        "  val_loader = DataLoader(dataset,\n",
        "                          batch_size = batch_size,\n",
        "                          sampler = validation_sampler,\n",
        "                          num_workers = num_workers)\n",
        "  return train_loader, val_loader"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXwd8gvh4GyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CAPTION PREPROCESSING FUNCTIONS\n",
        "from torchtext.data import Field\n",
        "from torchtext.data import TabularDataset\n",
        "from torchtext.data import Iterator\n",
        "\n",
        "def prepare_data(path, input_file, output_file):\n",
        "  input_path = os.path.join(path, input_file)\n",
        "  output_path = os.path.join(path, output_file)\n",
        "  df = pd.read_csv(input_path, sep = \"|\")\n",
        "  captions_array = df[' comment']\n",
        "  captions_array.fillna(\"\", inplace=True)\n",
        "  \n",
        "  image_names = df[\"image_name\"].values\n",
        "  image_number = []\n",
        "  for i in range(0, len(image_names)):\n",
        "    image_number.append(image_names[i].split('.')[0])\n",
        "\n",
        "  df.drop(labels=['image_name', ' comment_number'], axis=1, inplace=True)\n",
        "  df.index = image_number\n",
        "  df.to_csv(output_path, index_label=\"image_number\")\n",
        "  return output_path\n",
        "\n",
        "def build_vocab(path_to_caption_file, caption_file):\n",
        "  output_path = prepare_data(path_to_caption_file, caption_file, \"clean.csv\")\n",
        "  tokenize = lambda x : x.split()\n",
        "  TEXT = Field(sequential = True, tokenize = tokenize, lower=True, init_token='<start>', eos_token='<end>')\n",
        "  LABEL = Field(sequential=False, use_vocab=False)\n",
        "  td_datafields = [(\"image_number\", LABEL ),\n",
        "                  (\"comment\", TEXT)]\n",
        "\n",
        "  trn = TabularDataset(\n",
        "              path=output_path, # the root directory where the data lies\n",
        "              format='csv',\n",
        "              skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
        "              fields=td_datafields,\n",
        "              )\n",
        "  \n",
        "  TEXT.build_vocab(trn)\n",
        "  return TEXT.vocab\n",
        "\n",
        "def word_caption_to_index(path_to_caption_file, caption_file):\n",
        "  '''\n",
        "    This function takes as input the file containing the captions\n",
        "    and returns a matrix of the captions indexed with respect to the inner\n",
        "    vocabulary, as well as an array that can be used to map the indexed caption\n",
        "    to the image it belongs to.\n",
        "    @path_to_caption_file: the path to file containing the captions\n",
        "    @caption_file: the name of the file containing the captions\n",
        "  '''\n",
        "\n",
        "  output_path = prepare_data(path_to_caption_file, caption_file, \"clean.csv\")\n",
        "  tokenize = lambda x : x.split()\n",
        "  TEXT = Field(sequential = True, tokenize = tokenize, lower=True, init_token='<start>', eos_token='<end>')\n",
        "  LABEL = Field(sequential=False, use_vocab=False)\n",
        "\n",
        "  td_datafields = [(\"image_number\", LABEL ),\n",
        "                  (\"comment\", TEXT)]\n",
        "\n",
        "  trn = TabularDataset(\n",
        "                path=output_path, # the root directory where the data lies\n",
        "                format='csv',\n",
        "                skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
        "                fields=td_datafields\n",
        "                )\n",
        "  \n",
        "  TEXT.build_vocab(trn)\n",
        "  train_iter = Iterator(trn, batch_size=len(trn), device = -1)\n",
        "\n",
        "  for i in train_iter: #TODO:I really havent got it how this still works honestly\n",
        "    cmt = i.comment\n",
        "    img = i.image_number\n",
        "\n",
        "  return cmt.T, img\n",
        "\n",
        "def vocab_as_dict(path_to_caption_file, caption_file):\n",
        "  vocab = build_vocab(path_to_caption_file, caption_file)\n",
        "  return vocab.stoi\n",
        "\n",
        "def tensor_to_word(indexed_word, vocab):\n",
        "  for i in indexed_word:\n",
        "    k = i.item()\n",
        "    if k == 1:\n",
        "      break\n",
        "    else:\n",
        "      print(vocab.itos[k], end = \" \")\n",
        "    \n",
        "  \n",
        "\n",
        "def get_caption_from_image(caption_indexes, caption_refs, image_number):\n",
        "  #If image name actually has the .jpg tail\n",
        "  caption_refs = (caption_refs == int(image_number))\n",
        "  caption_refs = caption_refs.nonzero().T.numpy()[0].tolist()\n",
        "  return caption_indexes[caption_refs]\n",
        "\n",
        "def buildCaptionDict(path_to_caption_file, caption_file):\n",
        "\n",
        "  caption_index, refs = word_caption_to_index(path_to_caption_file, caption_file)\n",
        "  refs_list = refs.numpy().tolist()\n",
        "  refs_set = set(refs_list)\n",
        "  unique_refs_list = list(refs_set)\n",
        "\n",
        "  df = []\n",
        "  df = pd.DataFrame(columns=[\"Image_number\", \"Caption_1\", \"Caption_2\", \"Caption_3\", \"Caption_4\", \"Caption_5\"])\n",
        "\n",
        "  for i in range(0, len(unique_refs_list)):\n",
        "    capt_1 = get_caption_from_image(caption_index, refs, unique_refs_list[i])[0]\n",
        "    capt_2 = get_caption_from_image(caption_index, refs, unique_refs_list[i])[1]\n",
        "    capt_3 = get_caption_from_image(caption_index, refs, unique_refs_list[i])[2]\n",
        "    capt_4 = get_caption_from_image(caption_index, refs, unique_refs_list[i])[3]\n",
        "    capt_5 = get_caption_from_image(caption_index, refs, unique_refs_list[i])[4]\n",
        "    df.loc[i] = [unique_refs_list[i], capt_1, capt_2, capt_3, capt_4, capt_5]\n",
        "\n",
        "  df.set_index('Image_number', inplace=True)\n",
        "  capt_dict = df.to_dict('index')\n",
        "\n",
        "  return capt_dict"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syUMQ1Hs293g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2c888b56-4716-4153-9e5f-c92f8451c8a9"
      },
      "source": [
        "# TEST ON OUR DATA\n",
        "# image dictionary\n",
        "chunk1_dict = dict_from_shelve('/content/drive/My Drive/SML/SML_Project/Chunk1/img_shelve')\n",
        "\n",
        "# caption dictionary\n",
        "path_to_caption_file = \"/content/drive/My Drive/SML/SML_Project/Chunk1\"\n",
        "caption_file = \"results.csv\"\n",
        "caption_dict = buildCaptionDict(path_to_caption_file, caption_file)\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loHiiy6k7tFO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "outputId": "9a482eba-543a-47b4-bacb-0f711b507c41"
      },
      "source": [
        "dataset = imageCaptionDataset(chunk1_dict,caption_dict)\n",
        "#chunk1_dict['1624481']\n",
        "img_tensor, caption_tensor, size = dataset[1624481]\n",
        "print(img_tensor)\n",
        "print(caption_tensor)\n",
        "print(size)\n",
        "dataset.get_keys()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-0.7822, -0.6794, -0.6109,  ...,  0.2453,  0.4508,  0.8276],\n",
            "         [-0.8678, -0.8507, -0.8678,  ..., -0.6794, -0.5596, -0.4226],\n",
            "         [-0.8849, -0.8507, -0.8507,  ..., -0.7479, -0.7308, -0.7650],\n",
            "         ...,\n",
            "         [ 0.8447,  0.8447,  0.8276,  ...,  0.3309,  0.3481,  0.3481],\n",
            "         [ 0.9303,  0.9474,  0.9303,  ...,  0.3652,  0.3652,  0.3652],\n",
            "         [ 0.9132,  0.9303,  0.9303,  ...,  0.4166,  0.3823,  0.3652]],\n",
            "\n",
            "        [[-0.6352, -0.4951, -0.3901,  ...,  0.4678,  0.6779,  1.0455],\n",
            "         [-0.7227, -0.6702, -0.6527,  ..., -0.5476, -0.4251, -0.2675],\n",
            "         [-0.7402, -0.6702, -0.6527,  ..., -0.7402, -0.7227, -0.7227],\n",
            "         ...,\n",
            "         [ 1.0980,  1.0980,  1.0805,  ...,  0.5728,  0.5903,  0.5903],\n",
            "         [ 1.1856,  1.2031,  1.1856,  ...,  0.6078,  0.6078,  0.6078],\n",
            "         [ 1.1681,  1.1856,  1.1856,  ...,  0.6604,  0.6254,  0.6078]],\n",
            "\n",
            "        [[-0.6367, -0.5321, -0.4798,  ...,  0.2522,  0.4614,  0.8622],\n",
            "         [-0.7064, -0.7064, -0.6890,  ..., -0.6890, -0.5670, -0.4450],\n",
            "         [-0.7064, -0.6890, -0.6193,  ..., -0.7936, -0.7936, -0.8110],\n",
            "         ...,\n",
            "         [ 1.2457,  1.2457,  1.2282,  ...,  0.7228,  0.7402,  0.7402],\n",
            "         [ 1.3328,  1.3502,  1.3328,  ...,  0.7576,  0.7576,  0.7576],\n",
            "         [ 1.3154,  1.3328,  1.3328,  ...,  0.8099,  0.7751,  0.7576]]])\n",
            "tensor([   2,  198, 1106,   33,    6,    4,  270,   44, 1456,    6,   50,  533,\n",
            "          43,   40,    4,  417,  143,   13,    4,  276,   78,    6,    7,   88,\n",
            "           5,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1])\n",
            "torch.Size([84])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys([7340189, 134206, 5377361, 5771732, 4985704, 4199555, 1317156, 793558, 2760167, 10101477, 3680138, 3025093, 6827028, 6696219, 5648321, 667626, 8664920, 8664922, 3160699, 5914327, 8404753, 5521996, 148284, 9324151, 675153, 3035057, 5918675, 5918840, 807129, 5919020, 5526034, 1989609, 5791070, 5791244, 5791568, 6054169, 5400154, 9726060, 5402085, 3043766, 8680922, 4749855, 2784746, 9600569, 4489731, 8684718, 8029536, 2656351, 36979, 301246, 7510394, 6331511, 10002456, 960092, 6335241, 5287405, 438106, 178045, 6338704, 6338733, 6339096, 10010052, 10404007, 7520721, 7520731, 6734417, 574181, 4376178, 8832804, 1624481, 2148982, 4378823, 5558592, 2806447, 6214447, 7656601, 8443156, 7527111, 4906946, 2285664, 1369162, 4515460, 5958182, 4386588, 5566972, 10287332, 1243756, 3996401, 3734864, 5570219, 5570254, 65567, 8454235, 7013217, 984950, 8063007, 8849890, 2689611, 7015055, 854749, 10160966, 9637989, 5444724, 6100315, 726414, 4135695, 3219606, 205842, 7808046, 1254659, 4926723, 8859482, 8990627, 5060753, 8206921, 8601145, 81641, 3753939, 1920465, 6901333, 6901412, 6901479, 4280272, 3494059, 5722658, 4413714, 4414061, 2317271, 6905083, 3367399, 5333578, 7300624, 7300628, 5858851, 353913, 5729814, 9268793, 881336, 8876934, 2192573, 10188041, 3765589, 490870, 5733760, 3637013, 4162702, 5867606, 6261030, 2069887, 1283466, 4429660, 6920532, 5871970, 10459869, 5217116, 6659698, 8757787, 5088155, 6398924, 6398952, 6398988, 764507, 371897, 371903, 371902, 7188003, 6664030, 2209317, 2209751, 4307968, 2868798, 3001353, 3787801, 9556225, 9556226, 5230968, 5624522, 4183120, 4576671, 9950858, 9950913, 9426826, 10082347, 10082348, 8378599, 9035232, 5104045, 3662865, 5629300, 256063, 6678735, 6155176, 10350842, 7598674, 7598946, 390369, 3012229, 10090841, 3537322, 1440465])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_by1X2Gjxnk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO: Right now this is basically the same exact class described in the towards data science article\n",
        "#Should/Do we have to make any changes to this?\n",
        "class DecoderRNN(nn.Module): \n",
        "  def __init__(self, embed_size, hidden_size, vocab_size, num_layers = 1): #Here we define the layers\n",
        "    super().__init__()\n",
        "    self.embedding_captions_layer = nn.Embedding(vocab_size, embed_size)\n",
        "    self.LSTM = nn.LSTM(input_size = embed_size, hidden_size = hidden_size, \n",
        "                        num_layers = num_layers, batch_first = True)\n",
        "    self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "  def forward(self, features, captions): #Notice that features will only be used when we will have the encoder images in input\n",
        "    captions = captions[:, :-1]\n",
        "    embed = self.embedding_captions_layer(captions)\n",
        "    #A couple of comments here: torch.cat concatenates just like in unix\n",
        "    #torch.unsqueeze instead transforms the tensor into a column vector (column since we specify 1 here)\n",
        "    embed = torch.cat((features.unsqueeze(1), embed), dim = 1) \n",
        "    lstm_outputs, _ = self.LSTM(embed)\n",
        "    out = self.linear(lstm_outputs)\n",
        "    return out\n",
        "\n",
        "  def sample(self, inputs, states=None, max_len=20):\n",
        "    output_sentence = []\n",
        "\n",
        "    for i in range(max_len):\n",
        "      lstm_outputs, states = self.lstm(inputs, states)\n",
        "      lstm_outputs = lstm_outputs.squeeze(1)\n",
        "      out = self.linear(lstm_outputs)\n",
        "      last_pick = out.max(1)[1]\n",
        "      output_sentence.append(last_pick.item())\n",
        "      inputs = self.embedding_layer(last_pick).unsqueeze(1)\n",
        "      \n",
        "    return output_sentence"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CF7Q1WQvDhX-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ba9c866a-12f8-484f-a655-9647cfc2153c"
      },
      "source": [
        "#SOME TEST REGARDING SAMPLER AND DATALOADER\n",
        "\n",
        "# initialize a sampler\n",
        "#sampler = imageCaptionSampler(dataset)\n",
        "\n",
        "# initialize a dataloader using these 2 new classes\n",
        "#dataloader = DataLoader(dataset, batch_size=10, sampler=sampler, num_workers=1)\n",
        "\n",
        "# check if iteration works\n",
        "#dataiter = iter(dataloader)\n",
        "#print(dataiter.next())\n",
        "#print(dataiter.next()) \n",
        "#len(dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor([[[[ 2.2147,  2.2147,  2.2147,  ...,  2.2147,  2.2147,  2.2147],\n",
            "          [ 2.2147,  2.2147,  2.2147,  ...,  2.2147,  2.2147,  2.2147],\n",
            "          [ 2.2147,  2.2147,  2.2147,  ...,  2.2147,  2.2147,  2.2147],\n",
            "          ...,\n",
            "          [ 0.7591,  0.9303,  0.6221,  ...,  0.0227,  0.0056, -0.0972],\n",
            "          [ 1.1187,  0.8276,  0.9303,  ..., -0.2856, -0.1143, -0.0116],\n",
            "          [ 0.5536,  0.5878,  0.5364,  ...,  0.0569,  0.0912,  0.3652]],\n",
            "\n",
            "         [[ 2.3936,  2.3936,  2.3936,  ...,  2.3936,  2.3936,  2.3936],\n",
            "          [ 2.3936,  2.3936,  2.3936,  ...,  2.3936,  2.3936,  2.3936],\n",
            "          [ 2.3936,  2.3936,  2.3936,  ...,  2.3936,  2.3936,  2.3936],\n",
            "          ...,\n",
            "          [ 0.9580,  1.1331,  0.7829,  ...,  0.0651,  0.0651, -0.0049],\n",
            "          [ 1.3431,  1.0455,  1.0980,  ..., -0.2850, -0.1099,  0.0651],\n",
            "          [ 0.7829,  0.8004,  0.7129,  ...,  0.0651,  0.1001,  0.3978]],\n",
            "\n",
            "         [[ 2.6051,  2.6051,  2.6051,  ...,  2.6051,  2.6051,  2.6051],\n",
            "          [ 2.6051,  2.6051,  2.6051,  ...,  2.6051,  2.6051,  2.6051],\n",
            "          [ 2.6051,  2.6051,  2.6051,  ...,  2.6051,  2.6051,  2.6051],\n",
            "          ...,\n",
            "          [ 0.8099,  1.0191,  0.7751,  ...,  0.0953,  0.0431, -0.0790],\n",
            "          [ 1.1411,  0.8797,  1.0539,  ..., -0.2358, -0.0267,  0.0605],\n",
            "          [ 0.5136,  0.5485,  0.5834,  ...,  0.0605,  0.1651,  0.4614]]],\n",
            "\n",
            "\n",
            "        [[[-1.0048, -1.0904, -1.0562,  ..., -0.3541, -0.8164, -0.3712],\n",
            "          [-0.9534, -1.0390, -1.0048,  ...,  0.4851, -0.1314,  0.8789],\n",
            "          [-0.9877, -1.1589, -1.2959,  ...,  0.2111, -0.4911, -0.0972],\n",
            "          ...,\n",
            "          [-0.0801, -0.0972, -0.0629,  ..., -0.1143, -0.1143, -0.1143],\n",
            "          [-0.0458, -0.0801, -0.0801,  ..., -0.0287, -0.0458, -0.0458],\n",
            "          [-0.0972, -0.0972, -0.1314,  ...,  0.0569, -0.0116,  0.0056]],\n",
            "\n",
            "         [[-0.7402, -0.9328, -1.0378,  ..., -0.2850, -0.7927, -0.2850],\n",
            "          [-0.7752, -0.9328, -0.9503,  ...,  0.7129,  0.0826,  1.1331],\n",
            "          [-0.7752, -1.0203, -1.1779,  ...,  0.5903, -0.1625,  0.2052],\n",
            "          ...,\n",
            "          [ 0.5728,  0.5553,  0.5728,  ...,  0.5378,  0.5378,  0.5378],\n",
            "          [ 0.6078,  0.5728,  0.5728,  ...,  0.6078,  0.5903,  0.5903],\n",
            "          [ 0.5553,  0.5553,  0.5203,  ...,  0.6779,  0.6078,  0.6254]],\n",
            "\n",
            "         [[-0.4101, -0.5844, -0.5670,  ...,  0.3916, -0.1487,  0.3568],\n",
            "          [-0.4624, -0.6018, -0.5321,  ...,  1.3677,  0.6705,  1.6465],\n",
            "          [-0.3578, -0.6018, -0.7587,  ...,  1.1062,  0.3219,  0.6879],\n",
            "          ...,\n",
            "          [-0.3230, -0.3404, -0.2881,  ..., -0.2707, -0.2532, -0.2707],\n",
            "          [-0.2532, -0.2881, -0.2881,  ..., -0.1835, -0.2010, -0.2010],\n",
            "          [-0.3055, -0.3055, -0.3230,  ..., -0.1138, -0.1835, -0.1661]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0741,  0.0741,  0.0912,  ..., -0.7650, -1.0048, -1.2103],\n",
            "          [ 0.0398,  0.0398,  0.0741,  ..., -0.7308, -0.9363, -1.1932],\n",
            "          [ 0.1083,  0.0912,  0.0741,  ..., -0.8507, -0.9192, -1.2788],\n",
            "          ...,\n",
            "          [-0.0116, -0.1314, -0.1999,  ...,  0.5707,  0.7419,  0.7248],\n",
            "          [-0.2342, -0.0972, -0.0629,  ...,  0.3994,  0.6563,  0.7933],\n",
            "          [-0.1486, -0.0116, -0.1828,  ...,  0.3823,  0.6392,  0.6734]],\n",
            "\n",
            "         [[ 0.3102,  0.3102,  0.3102,  ..., -0.7577, -1.2304, -1.4405],\n",
            "          [ 0.2752,  0.2752,  0.2927,  ..., -0.6877, -1.1078, -1.4230],\n",
            "          [ 0.2927,  0.2927,  0.2752,  ..., -0.7927, -0.9853, -1.4755],\n",
            "          ...,\n",
            "          [-0.0224, -0.1800, -0.2500,  ...,  0.6078,  0.7829,  0.7129],\n",
            "          [-0.2675, -0.1450, -0.1099,  ...,  0.4328,  0.6954,  0.7654],\n",
            "          [-0.1450, -0.0224, -0.1800,  ...,  0.4153,  0.6429,  0.6078]],\n",
            "\n",
            "         [[ 0.4788,  0.5136,  0.6008,  ..., -0.5495, -1.0898, -1.2816],\n",
            "          [ 0.4439,  0.4788,  0.5659,  ..., -0.4973, -0.9678, -1.2293],\n",
            "          [ 0.5311,  0.4962,  0.4788,  ..., -0.5495, -0.7936, -1.3339],\n",
            "          ...,\n",
            "          [-0.0441, -0.1661, -0.2532,  ...,  0.5136,  0.6879,  0.6531],\n",
            "          [-0.2707, -0.1312, -0.1138,  ...,  0.2696,  0.5659,  0.6879],\n",
            "          [-0.1661, -0.0267, -0.2184,  ...,  0.2696,  0.5136,  0.5311]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.1314, -0.1828, -0.2342,  ..., -0.2171, -0.3027, -0.3027],\n",
            "          [-0.1999, -0.1657, -0.2171,  ..., -0.3027, -0.3027, -0.2342],\n",
            "          [-0.1999, -0.1999, -0.2171,  ..., -0.2171, -0.1828, -0.1657],\n",
            "          ...,\n",
            "          [-0.6452, -0.7650, -0.7650,  ..., -0.7308, -0.6794, -0.4568],\n",
            "          [-0.7479, -0.6281, -0.6965,  ..., -0.4397, -0.6109, -0.5767],\n",
            "          [-0.7137, -0.5767, -0.6965,  ..., -0.5424, -0.5253, -0.5424]],\n",
            "\n",
            "         [[ 0.0126, -0.0224, -0.0574,  ..., -0.0749, -0.1625, -0.1625],\n",
            "          [-0.0574, -0.0049, -0.0224,  ..., -0.1625, -0.1625, -0.0924],\n",
            "          [-0.0574, -0.0399, -0.0224,  ..., -0.0749, -0.0399, -0.0224],\n",
            "          ...,\n",
            "          [-0.5476, -0.6702, -0.6352,  ..., -0.6527, -0.6001, -0.3725],\n",
            "          [-0.6527, -0.5301, -0.5826,  ..., -0.3550, -0.5301, -0.4951],\n",
            "          [-0.6176, -0.4776, -0.6001,  ..., -0.4951, -0.4776, -0.4951]],\n",
            "\n",
            "         [[ 0.3045,  0.2696,  0.2173,  ...,  0.2173,  0.1476,  0.1476],\n",
            "          [ 0.2348,  0.2696,  0.2522,  ...,  0.1302,  0.1302,  0.1999],\n",
            "          [ 0.2348,  0.2348,  0.2522,  ...,  0.2173,  0.2522,  0.2696],\n",
            "          ...,\n",
            "          [-0.3404, -0.5147, -0.4973,  ..., -0.4101, -0.3578, -0.1312],\n",
            "          [-0.4450, -0.3578, -0.4275,  ..., -0.1138, -0.2881, -0.2532],\n",
            "          [-0.4275, -0.2707, -0.3753,  ..., -0.2881, -0.2532, -0.2707]]],\n",
            "\n",
            "\n",
            "        [[[ 1.5468,  1.5125,  1.4954,  ...,  0.6563,  0.6392,  0.6392],\n",
            "          [ 1.5639,  1.5810,  1.5982,  ...,  0.7248,  0.5536,  0.7077],\n",
            "          [ 1.5810,  1.6153,  1.6153,  ...,  0.7933,  0.6734,  0.7762],\n",
            "          ...,\n",
            "          [-2.0152, -2.0152, -1.9638,  ..., -1.9295, -1.9124, -1.8953],\n",
            "          [-2.0152, -2.0323, -1.9638,  ..., -1.9295, -1.8953, -1.8953],\n",
            "          [-1.9809, -2.0323, -1.9809,  ..., -1.9295, -1.9124, -1.8953]],\n",
            "\n",
            "         [[ 1.2906,  1.2556,  1.2381,  ..., -0.1800, -0.1975, -0.1800],\n",
            "          [ 1.4132,  1.3782,  1.3256,  ..., -0.1275, -0.2850, -0.1450],\n",
            "          [ 1.4482,  1.4132,  1.3606,  ..., -0.1099, -0.2500, -0.1450],\n",
            "          ...,\n",
            "          [-1.8606, -1.8431, -1.8256,  ..., -1.8431, -1.8256, -1.8081],\n",
            "          [-1.8606, -1.8606, -1.8256,  ..., -1.8606, -1.8256, -1.8081],\n",
            "          [-1.8431, -1.8782, -1.8431,  ..., -1.8431, -1.8256, -1.8081]],\n",
            "\n",
            "         [[ 1.7511,  1.7337,  1.6640,  ..., -0.7064, -0.7238, -0.7238],\n",
            "          [ 1.8034,  1.8208,  1.7685,  ..., -0.7064, -0.8458, -0.6715],\n",
            "          [ 1.8383,  1.8557,  1.7860,  ..., -0.6890, -0.7936, -0.6890],\n",
            "          ...,\n",
            "          [-1.6476, -1.6476, -1.6127,  ..., -1.5604, -1.5604, -1.5779],\n",
            "          [-1.6650, -1.6476, -1.6127,  ..., -1.5430, -1.5430, -1.5430],\n",
            "          [-1.6302, -1.6476, -1.6127,  ..., -1.5604, -1.5604, -1.5604]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0569,  0.1426,  0.0227,  ..., -1.6213, -1.6213, -1.6555],\n",
            "          [-0.9705, -0.8164, -0.8507,  ..., -1.9124, -1.8953, -1.8439],\n",
            "          [-1.5357, -1.5528, -1.6384,  ..., -1.6042, -1.6213, -1.6384],\n",
            "          ...,\n",
            "          [-1.8953, -1.8268, -1.9638,  ...,  0.0227,  0.0398,  0.0227],\n",
            "          [-1.9124, -1.8610, -1.9467,  ..., -0.0287, -0.1143, -0.0801],\n",
            "          [-1.8782, -1.9980, -1.9295,  ..., -0.0972, -0.1143, -0.0458]],\n",
            "\n",
            "         [[ 0.6954,  0.8004,  0.6954,  ..., -1.0903, -1.1078, -1.1429],\n",
            "          [-0.6001, -0.4426, -0.4776,  ..., -1.6506, -1.6856, -1.6155],\n",
            "          [-1.0378, -1.0728, -1.1954,  ..., -1.3354, -1.3004, -1.3179],\n",
            "          ...,\n",
            "          [-1.1954, -1.1429, -1.3004,  ...,  0.5028,  0.5903,  0.5553],\n",
            "          [-1.2129, -1.1954, -1.3529,  ...,  0.4503,  0.4328,  0.4503],\n",
            "          [-1.1954, -1.3179, -1.1954,  ...,  0.3803,  0.3978,  0.4503]],\n",
            "\n",
            "         [[ 0.6531,  0.7576,  0.6531,  ..., -1.1073, -1.1247, -1.1596],\n",
            "          [-0.5495, -0.3927, -0.4275,  ..., -1.5081, -1.5081, -1.4559],\n",
            "          [-0.9504, -0.9853, -1.0898,  ..., -1.1944, -1.1770, -1.1770],\n",
            "          ...,\n",
            "          [-0.6367, -0.5844, -0.7761,  ...,  0.6531,  0.7054,  0.6705],\n",
            "          [-0.6890, -0.6715, -0.8458,  ...,  0.6008,  0.5659,  0.5659],\n",
            "          [-0.6715, -0.8110, -0.7064,  ...,  0.5311,  0.5311,  0.5834]]]]), tensor([[    2,     4,    10,     6,     4,    30,    25,    11,    44,    51,\n",
            "             4,   783,    27,    76,    10,  1158,   120,    13,     4,    26,\n",
            "          3829,     5,     3,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1],\n",
            "        [    2,     4,    85,  3263,     8,     4,   173,   245,     5,     3,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1],\n",
            "        [    2,     4,    34,     6,     4,   122,  5153,  6090,     4, 10760,\n",
            "           141,     5,     3,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1],\n",
            "        [    2,     4,   157,  2356,   357,    22,  1449,  4577,     8,    28,\n",
            "          3614,   430,   119,  2299,    28,   188,     5,     3,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1],\n",
            "        [    2,     4,    10,     8,     4, 19413,  1571,  2598,     4,    85,\n",
            "            12,  8003,   550,     5,     3,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1],\n",
            "        [    2,     4,    10,     6,     4,    29,    25,    13,    24,  1310,\n",
            "            14,    95,     8,     7,   150,    12,     7,   143,    13,    28,\n",
            "          2792,    18,    28,   188,    47,     4,  1910,   331,    12,   521,\n",
            "             9,   667,     5,     3,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1],\n",
            "        [    2,    21,   273,   503,   289,    11,  1498,     6,    41,    12,\n",
            "            16,    69,  1548,   459,     5,     3,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1],\n",
            "        [    2,     4,    56,    31,  2580,     4,    56,    34,   109,  1129,\n",
            "             6,     7,    38,     5,     3,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1],\n",
            "        [    2,    33,     6,    30,  5799,    17,   263,     6,     4,  1350,\n",
            "             5,     3,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1],\n",
            "        [    2,     4,    10,     6,     4,    24,     9,    29,  2969,    11,\n",
            "           208,     4,  1319,    18,  3723,   975,     5,     3,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1]]), [tensor([84, 84, 84, 84, 84, 84, 84, 84, 84, 84])]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjvEY6Ip1eIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#PARAMETERS DEFINITION\n",
        "\n",
        "\n",
        "#from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "# RNN - Model parameters\n",
        "hidden_size = 100\n",
        "embed_size = 50\n",
        "vocab_size = len(vocab_as_dict('/content/drive/My Drive/SML/SML_Project/Chunk1/', 'results.csv'))\n",
        "dropout = 0.5 # suggested\n",
        "\n",
        "# Parameters\n",
        "epoch = 2\n",
        "start_epoch = 0\n",
        "epochs_since_improvement = 0 # keeps track of number of epochs since there's been an improvement in validation BLEU\n",
        "batch_size = 32\n",
        "decoder_lr = 1e-4  # learning rate\n",
        "checkpoint = None\n",
        "bleu4 = 0. # bleu score -> https://www.aclweb.org/anthology/P02-1040.pdf\n",
        "workers = 1 # for dataloaders: how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehazjG4c1mN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# maybe this should be saved in a utils.py file\n",
        "\n",
        "def adjust_learning_rate(optimizer, shrink_factor):\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = param_group['lr'] * shrink_factor\n",
        "\n",
        "def clip_gradient(optimizer, grad_clip):\n",
        "  for i in optimizer.param_groups:\n",
        "    for param in group['params']:\n",
        "      if param.grad is not None:\n",
        "        param.grad.data.clamp_(-grad_clip, grad_clip)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifWMIAWg2OgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_loader, encoder, decoder, criterion, decoder_optimizer, epoch, device, grad_clip):\n",
        "\n",
        "  decoder.train() # train mode\n",
        "\n",
        "  # Epochs\n",
        "  for i in range(epoch):\n",
        "    \n",
        "    # Batch\n",
        "    for i, (image, caption, captionlen) in enumerate(train_loader):\n",
        "      \n",
        "      # Move to GPU, if available\n",
        "      image = image.to(device)\n",
        "      caption = caption.to(device)\n",
        "      #captionlen = captionlen.to(device)\n",
        "\n",
        "      # Output encoder\n",
        "      image = encoder(image)\n",
        "\n",
        "      # Forward \n",
        "      decoder_output = decoder(image, caption)\n",
        "      \n",
        "      \n",
        "      # Calculate loss\n",
        "      loss = criterion(decoder_output.view(-1, vocab_size), caption.view(-1))\n",
        "      #loss = criterion(decoder_output, caption)\n",
        "      print('Loss = ', loss)  \n",
        "\n",
        "      # Back propagation\n",
        "      decoder_optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      \n",
        "\n",
        "      # Clip gradients\n",
        "      # https://machinelearningmastery.com/how-to-avoid-exploding-gradients-in-neural-networks-with-gradient-clipping/\n",
        "      if grad_clip is not None:\n",
        "        clip_gradient(decoder_optimizer, grad_clip)\n",
        "\n",
        "      # Update weights\n",
        "      decoder_optimizer.step()\n",
        "\n",
        "def test()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aITyb41r1oMG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "ae345f70-bd82-44c4-df14-5f362e32e3e1"
      },
      "source": [
        "# Encoder\n",
        "encoder = EncoderCNN(embed_size)\n",
        "\n",
        "# Decoder\n",
        "decoder = DecoderRNN(embed_size, hidden_size, vocab_size) #dropout = dropout)\n",
        "\n",
        "decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, \n",
        "                                                   decoder.parameters()),\n",
        "                                     lr=decoder_lr)\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# DataLoader\n",
        "train_loader, val_loader = loaders(dataset, 0.2, batch_size, 1)\n",
        "\n",
        "'''\n",
        "# Epochs\n",
        "for epoch in range(start_epoch, epochs):\n",
        "  # Decay learning rate if there is no improvement for 8 consecutive epochs, and terminate training after 20\n",
        "  if epochs_since_improvement == 20:\n",
        "    break\n",
        "  if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n",
        "    adjust_learning_rate(decoder_optimizer, 0.8)\n",
        "'''\n",
        "\n",
        "# Training\n",
        "train(train_loader=train_loader,\n",
        "      encoder=encoder,\n",
        "      decoder=decoder,\n",
        "      criterion=criterion,\n",
        "      decoder_optimizer=decoder_optimizer,\n",
        "      epoch=epoch,\n",
        "      device=device,\n",
        "      grad_clip=None)\n",
        "\n",
        "  # Validation\n",
        "  #bleu4_new\n",
        "\n",
        "'''\n",
        "  # BLEU4\n",
        "  flag1 = bleu4_new > bleu4\n",
        "  bleu4 = max(bleu4_new, bleu4)\n",
        "  if not flag1:\n",
        "    epochs_since_improvement += 1\n",
        "  else:\n",
        "    epochs_since_improvement = 0\n",
        "'''"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss =  tensor(9.9713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Loss =  tensor(9.9555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Loss =  tensor(9.9405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Loss =  tensor(9.9248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Loss =  tensor(9.9101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Loss =  tensor(9.8944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Loss =  tensor(9.8783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Loss =  tensor(9.8638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Loss =  tensor(9.8501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Loss =  tensor(9.8351, device='cuda:0', grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n  # BLEU4\\n  flag1 = bleu4_new > bleu4\\n  bleu4 = max(bleu4_new, bleu4)\\n  if not flag1:\\n    epochs_since_improvement += 1\\n  else:\\n    epochs_since_improvement = 0\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2OihxLZiKMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}