{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StillExploringSMLExam_drive.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1IYWvmCYZV9rtDGGyAz6yn01gQ1y9ngLE",
      "authorship_tag": "ABX9TyMcG9+Nzs4eNdX4v3yumXaM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/earendil94/SMLExam/blob/master/CaptionPreProcessing_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T879TdgecL9H",
        "colab_type": "text"
      },
      "source": [
        "## A quick overview\n",
        "\n",
        "**Q) What still needs to be done regarding the RNN?**\n",
        "\n",
        "A) We still need to find a proper way to sample an image and a caption.\n",
        "I wanted to try to get the RNN to work alone but I am missing the proper\n",
        "label if I use the text as an input. Moreover we need to define the learning procedure of our model\n",
        "\n",
        "#### Useful stuff\n",
        "https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/\n",
        "\n",
        "https://towardsdatascience.com/automatic-image-captioning-with-cnn-rnn-aae3cd442d83\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1suQNxtqGCD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import pandas as pd\n",
        "from spacy.lang.en import English #I am not even sure we need it. In fact I believe we don't\n",
        "import en_core_web_sm"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er7p5w3-uVJo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e99962a3-9437-45e2-a5a4-2a413065d810"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3zcxf7ffBgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading captions file and creating the array\n",
        "\n",
        "#TODO: this is going to be very rough for now\n",
        "#Later:\n",
        "#  Split our dataset into training/test\n",
        "#  add a min number of repetitions in the training data to discard rare words \n",
        "\n",
        "import os\n",
        "\n",
        "def prepare_data(path, input_file, output_file):\n",
        "  input_path = os.path.join(path, input_file)\n",
        "  output_path = os.path.join(path, output_file)\n",
        "  df = pd.read_csv(input_path, sep = \"|\")\n",
        "  captions_array = df[' comment']\n",
        "  captions_array.fillna(\"\", inplace=True)\n",
        "  \n",
        "  image_names = df[\"image_name\"].values\n",
        "  image_number = []\n",
        "  for i in range(0, len(image_names)):\n",
        "    image_number.append(image_names[i].split('.')[0])\n",
        "\n",
        "  df.drop(labels=['image_name', ' comment_number'], axis=1, inplace=True)\n",
        "  df.index = image_number\n",
        "  df.to_csv(output_path, index_label=\"image_number\")\n",
        "\n",
        "\n",
        "prepare_data(\"/content/drive/My Drive/SML_Project/Chunk1\", \"results.csv\", \"clean.csv\")"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HtZ29oz2vUS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "27d91ad4-bae9-49af-e499-fbc00c71ba27"
      },
      "source": [
        "from torchtext.data import Field\n",
        "from torchtext.data import TabularDataset\n",
        "\n",
        "tokenize = lambda x : x.split()\n",
        "TEXT = Field(sequential = True, tokenize = tokenize, lower=True, init_token='<start>', eos_token='<end>')\n",
        "LABEL = Field(sequential=False, use_vocab=False)\n",
        "\n",
        "td_datafields = [(\"image_number\", LABEL ),\n",
        "                 (\"comment\", TEXT)]\n",
        "\n",
        "trn = TabularDataset(\n",
        "              path=\"/content/drive/My Drive/SML_Project/Chunk1/clean.csv\", # the root directory where the data lies\n",
        "              format='csv',\n",
        "              skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
        "              fields=td_datafields,\n",
        "              #csv_reader_params={'delimiter':'\\t'}\n",
        "              )\n",
        "\n",
        "#Build the vocabulary\n",
        "trn.examples[0].comment"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['two',\n",
              " 'young',\n",
              " 'guys',\n",
              " 'with',\n",
              " 'shaggy',\n",
              " 'hair',\n",
              " 'look',\n",
              " 'at',\n",
              " 'their',\n",
              " 'hands',\n",
              " 'while',\n",
              " 'hanging',\n",
              " 'out',\n",
              " 'in',\n",
              " 'the',\n",
              " 'yard',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3GR1syQ4kvg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "aaeb593a-c4e7-46de-f8b8-247619758acb"
      },
      "source": [
        "#Building the vocabolary\n",
        "TEXT.build_vocab(trn)\n",
        "print(len(trn))\n",
        "TEXT.vocab.itos[1]"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "158915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXl2kACgRw-G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "5ce54f1c-9f7c-4adf-9140-b76aa7386c3c"
      },
      "source": [
        "from torchtext.data import Iterator\n",
        "train_iter = Iterator(trn, batch_size=len(trn), device = -1)\n",
        "\n",
        "\n",
        "for i in train_iter:\n",
        "  cmt = i.comment\n",
        "  img = i.image_number\n",
        "\n",
        "cmt.T\n",
        "\n"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  2,   4,  10,  ...,   1,   1,   1],\n",
              "        [  2,   4, 853,  ...,   1,   1,   1],\n",
              "        [  2, 104,  15,  ...,   1,   1,   1],\n",
              "        ...,\n",
              "        [  2,   4, 172,  ...,   1,   1,   1],\n",
              "        [  2,  16,  15,  ...,   1,   1,   1],\n",
              "        [  2,   4,  69,  ...,   1,   1,   1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFXZWEW8Dx_o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "c26e1099-3e0a-4d06-83b6-10130fd509d3"
      },
      "source": [
        "caption_indexed = cmt.T\n",
        "print(caption_indexed[0])\n",
        "\n",
        "for i in caption_indexed[0]:\n",
        "  k = i.item()\n",
        "  if k == 1:\n",
        "    break\n",
        "  print(TEXT.vocab.itos[k], end = \" \")\n",
        "\n",
        "print(\"\\n\", img[0].item())\n",
        "\n",
        "\n",
        "for i in caption_indexed[1]:\n",
        "  k = i.item()\n",
        "  if k == 1:\n",
        "    break\n",
        "  print(TEXT.vocab.itos[k], end = \" \")\n",
        "\n",
        "print(\"\\n\", img[1].item())\n",
        "\n",
        "\n",
        "caption_map = {}\n",
        "\n",
        "#keys = \n",
        "\n",
        "#for i in range(1, len(trn)):\n",
        "\n"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([   2,    4,   10,    6,    4,   26,  281,   95,   20,    4,  602, 3125,\n",
            "           8,    4,  428, 1050,    5,    3,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1])\n",
            "<start> a man in a black sweater sits at a desk typing on a computer keyboard . <end> \n",
            " 2203168780\n",
            "<start> a singer in gray suit and black hat holds a guitar and sings into a microphone at an outside concert . <end> \n",
            " 4648495016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQqUvSDyHYp6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e78585c0-3467-4eb4-a0c4-43ad95453a6f"
      },
      "source": [
        "#Actual run devices\n",
        "embed_size = 100\n",
        "hidden_size = 100\n",
        "epochs = 3\n",
        "path = \"/content/drive/My Drive/SML_Project/Chunk1/results.csv\"\n",
        "learning_rate = 0.01\n",
        "\n",
        "\n",
        "vocab_size = load_vocab(path=path)\n",
        "decoder = DecoderRNN(embed_size=embed_size, hidden_size=hidden_size, vocab_size=vocab_size)\n",
        "device = \"cpu\"\n",
        "\n",
        "decoder.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "params = decoder.parameters()\n",
        "optimizer = optim.Adam(lr = learning_rate)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(2, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIMC6SuBp2ew",
        "colab_type": "text"
      },
      "source": [
        "# THIS IS THE ACTUAL PART WE ARE INTERESTED IN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3jjRPNmqDEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import pandas as pd\n",
        "\n",
        "#TODO: Right now this is basically the same exact class described in the towards data science article\n",
        "#Should/Do we have to make any changes to this?\n",
        "class DecoderRNN(nn.Module): \n",
        "  def __init__(self, embed_size, hidden_size, vocab_size, num_layers = 1): #Here we define the layers\n",
        "    super().__init__()\n",
        "    self.embedding_captions_layer = nn.Embedding(vocab_size, embed_size)\n",
        "    self.LSTM = nn.LSTM(input_size = embed_size, hidden_size = hidden_size, \n",
        "                        num_layers = num_layers, batch_first = TRUE)\n",
        "    self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "  def forward(self, features, captions): #Notice that features will only be used when we will have the encoder images in input\n",
        "    #captions = captions[:, :-1]\n",
        "    embed = self.embedding_layer(captions)\n",
        "    #A couple of comments here: torch.cat concatenates just like in unix\n",
        "    #torch.unsqueeze instead transforms the tensor into a column vector (column since we specify 1 here)\n",
        "    #embed = torch.cat((features.unsqueeze(1), embed), dim = 1) \n",
        "    lstm_outputs, _ = self.lstm(embed)\n",
        "    out = self.linear(lstm_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnLqDPGJCHgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To put into a module\n",
        "from torchtext.data import Field\n",
        "from torchtext.data import TabularDataset\n",
        "from torchtext.data import Iterator\n",
        "\n",
        "def prepare_data(path, input_file, output_file):\n",
        "  input_path = os.path.join(path, input_file)\n",
        "  output_path = os.path.join(path, output_file)\n",
        "  df = pd.read_csv(input_path, sep = \"|\")\n",
        "  captions_array = df[' comment']\n",
        "  captions_array.fillna(\"\", inplace=True)\n",
        "  \n",
        "  image_names = df[\"image_name\"].values\n",
        "  image_number = []\n",
        "  for i in range(0, len(image_names)):\n",
        "    image_number.append(image_names[i].split('.')[0])\n",
        "\n",
        "  df.drop(labels=['image_name', ' comment_number'], axis=1, inplace=True)\n",
        "  df.index = image_number\n",
        "  df.to_csv(output_path, index_label=\"image_number\")\n",
        "  return output_path\n",
        "\n",
        "def build_vocab(path_to_caption_file, caption_file):\n",
        "  output_path = prepare_data(path_to_caption_file, caption_file, \"clean.csv\")\n",
        "  tokenize = lambda x : x.split()\n",
        "  TEXT = Field(sequential = True, tokenize = tokenize, lower=True, init_token='<start>', eos_token='<end>')\n",
        "  td_datafields = [(\"image_number\", LABEL ),\n",
        "                  (\"comment\", TEXT)]\n",
        "\n",
        "  trn = TabularDataset(\n",
        "              path=output_path, # the root directory where the data lies\n",
        "              format='csv',\n",
        "              skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
        "              fields=td_datafields,\n",
        "              )\n",
        "  \n",
        "  TEXT.build_vocab(trn)\n",
        "  return TEXT.vocab\n",
        "\n",
        "def word_caption_to_index(path_to_caption_file, caption_file):\n",
        "  '''\n",
        "    This function takes as input the file containing the captions\n",
        "    and returns a matrix of the captions indexed with respect to the inner\n",
        "    vocabulary, as well as an array that can be used to map the indexed caption\n",
        "    to the image it belongs to.\n",
        "    @path_to_caption_file: the path to file containing the captions\n",
        "    @caption_file: the name of the file containing the captions\n",
        "  '''\n",
        "\n",
        "  output_path = prepare_data(path_to_caption_file, caption_file, \"clean.csv\")\n",
        "  tokenize = lambda x : x.split()\n",
        "  TEXT = Field(sequential = True, tokenize = tokenize, lower=True, init_token='<start>', eos_token='<end>')\n",
        "  LABEL = Field(sequential=False, use_vocab=False)\n",
        "\n",
        "  td_datafields = [(\"image_number\", LABEL ),\n",
        "                  (\"comment\", TEXT)]\n",
        "\n",
        "  trn = TabularDataset(\n",
        "                path=output_path, # the root directory where the data lies\n",
        "                format='csv',\n",
        "                skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
        "                fields=td_datafields\n",
        "                )\n",
        "  \n",
        "  TEXT.build_vocab(trn)\n",
        "  train_iter = Iterator(trn, batch_size=len(trn), device = -1)\n",
        "\n",
        "  for i in train_iter: #TODO:I really havent got it how this still works honestly\n",
        "    cmt = i.comment\n",
        "    img = i.image_number\n",
        "\n",
        "  return cmt.T, img\n",
        "\n",
        "def vocab_as_dict(path_to_caption_file, caption_file):\n",
        "  vocab = build_vocab(path_to_caption_file, caption_file)\n",
        "  return vocab.stoi\n",
        "\n",
        "def tensor_to_word(indexed_word, vocab):\n",
        "  for i in indexed_word:\n",
        "    k = i.item()\n",
        "    if k == 1:\n",
        "      break\n",
        "    else:\n",
        "      print(vocab.itos[k], end = \" \")\n",
        "    \n",
        "  \n",
        "\n",
        "def get_caption_from_image(caption_indexes, caption_refs, image_name):\n",
        "  #If image name actually has the .jpg tail\n",
        "  image_number = image_name.split('.')[0]\n",
        "  caption_refs = (caption_refs == int(image_number))\n",
        "  caption_refs = caption_refs.nonzero().T.numpy()[0].tolist()\n",
        "  return caption_indexed[caption_refs]"
      ],
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "safmGuUKqAVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdqcytxOMCNR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d3fc8f5d-1303-402e-c7c0-15b20cb39269"
      },
      "source": [
        "caption_index, refs = word_caption_to_index(\"/content/drive/My Drive/SML_Project/Chunk1\", \"results.csv\")\n",
        "vox = build_vocab(\"/content/drive/My Drive/SML_Project/Chunk1\", \"results.csv\")"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGjLw6DTdWvT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 844
        },
        "outputId": "bf3d17f5-2ef1-41cc-d8f8-9d4e987241d8"
      },
      "source": [
        "caps = get_caption_from_image(caption_index, refs, '1000092795.jpg')\n",
        "print(caps)"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[    2,     4,    10,     6,     4,    29,    25,    35,     6,     4,\n",
            "           715,     5,     3,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1],\n",
            "        [    2,    16,   487,   816,   599, 12070,   140,     5,     3,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1],\n",
            "        [    2,    16,    33,     6,    50,   258,    17,    35,     6,     4,\n",
            "           492,     5,     3,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1],\n",
            "        [    2,    16,    23,   327,    13,  2134,   112,   189,    20,    66,\n",
            "           161,    27,   326,    75,     6,     7,   492,     5,     3,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1],\n",
            "        [    2,    16,    23,    14,    24,   711,    17,    58,    83,   198,\n",
            "          1431,     5,     3,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiAmt1yYdvCa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8898c183-a0d9-432e-a0a6-b628036558ae"
      },
      "source": [
        "tensor_to_word(caps[2], vox)"
      ],
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> two men in green shirts are standing in a yard . <end> "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5DEWZ4ul8YE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}